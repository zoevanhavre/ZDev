library(servr)
?jekyll
![](https://media3.giphy.com/media/l2JHPriZtl8lCOwjS/200w.gif)
The main reference can be found
library(Zmix)
y=c(1:10)#
k=2#
iter=50#
tau=0.01#
isSim=FALSE
if(isSim==TRUE) {Y<-y$Y#
					}else{ Y<-y}#
				parallelAccept<-function(w1, w2, a1, a2){#
						w1[w1< 1e-200]<-1e-200             # truncate so super small values dont crash everyting#
						w2[w2< 1e-200]<-1e-200#
						T1<-dDirichlet(w2, a1, log=TRUE)#
						T2<-dDirichlet(w1, a2, log=TRUE)#
						B1<-dDirichlet(w1, a1, log=TRUE)#
						B2<-dDirichlet(w2, a2, log=TRUE)#
						MH<-min(1,	exp(T1+T2-B1-B2))#
						Ax<-sample(c(1,0), 1, prob=c(MH,1-MH))#
						return(Ax)}#
					nCh<-length(alphas)#
						TrackParallelTemp<-matrix(nrow=iter, ncol=nCh)#
						TrackParallelTemp[1,]<-c(1:nCh)#
					#tau=1#
					n <-length(Y)#
					a=2.5; b=2/var(Y)#
					d<-2#
					lambda=sum(Y)/n#
			                                                                                                          # 1. set up priors#
					mux<-list(mu=seq(from=min(Y), to=max(Y),length.out=k),sigma=rep(1, k),p=rep(1/k,k), k=k)#
					n <-length(Y)#
					a=2.5; b<-0.5*var(Y);d<-2#
					lambda<-sum(Y)/n  ;#
					pb <- txtProgressBar(min = 0, max = iter, style = 3)#
#
			                                                                                                          # 2. set up matrices for parameters which will be saved#
					map =    matrix(0,nrow = iter, ncol = 1)#
					Loglike =   matrix(0,nrow = iter, ncol = 1)#
					Bigmu = replicate(nCh,  matrix(0,nrow = iter, ncol = k)	, simplify=F)#
					Bigsigma=replicate(nCh,  matrix(0,nrow = iter, ncol = k)	, simplify=F)#
					Bigp =  replicate(nCh,  matrix(0,nrow = iter, ncol = k)	, simplify=F)#
					Pzs =   replicate(nCh,  matrix(0,nrow = n, ncol = k)	, simplify=F)#
					ZSaved=	replicate(nCh,  matrix(0,nrow = n, ncol = iter)	, simplify=F)#
					SteadyScore<-data.frame("Iteration"=c(1:iter), "K0"=k)#
			                                                                                                          # start chains and  create inits needed for i=1#
					for (.ch in 1:nCh){#
					Bigmu[[.ch]][1,] <- mux$mu                                                                        # initial value of mu's#
					mu0=mux$mu#
					Bigp[[.ch]][1,] = mux$p                                                                           # inital value of p's#
					p0=mux$p#
					Bigsigma[[.ch]][1,] = mux$sigma                                                                   # inits for sigma#
					sig0=mux$sigma	}#
#
			                                                                                                          #  		initialize chains:  ie. iteration 1:#
					j<-1
alphas=  1/2^(20)
alphas
if(isSim==TRUE) {Y<-y$Y#
#
					}else{ Y<-y}#
#
				parallelAccept<-function(w1, w2, a1, a2){#
#
						w1[w1< 1e-200]<-1e-200             # truncate so super small values dont crash everyting#
#
						w2[w2< 1e-200]<-1e-200#
#
						T1<-dDirichlet(w2, a1, log=TRUE)#
#
						T2<-dDirichlet(w1, a2, log=TRUE)#
#
						B1<-dDirichlet(w1, a1, log=TRUE)#
#
						B2<-dDirichlet(w2, a2, log=TRUE)#
#
						MH<-min(1,	exp(T1+T2-B1-B2))#
#
						Ax<-sample(c(1,0), 1, prob=c(MH,1-MH))#
#
						return(Ax)}#
					nCh<-length(alphas)#
#
						TrackParallelTemp<-matrix(nrow=iter, ncol=nCh)#
#
						TrackParallelTemp[1,]<-c(1:nCh)
#tau=1#
#
					n <-length(Y)#
#
					a=2.5; b=2/var(Y)#
#
					d<-2#
#
					lambda=sum(Y)/n#
#
			                                                                                                          # 1. set up priors#
#
					mux<-list(mu=seq(from=min(Y), to=max(Y),length.out=k),sigma=rep(1, k),p=rep(1/k,k), k=k)#
#
					n <-length(Y)#
#
					a=2.5; b<-0.5*var(Y);d<-2#
#
					lambda<-sum(Y)/n  ;#
#
					pb <- txtProgressBar(min = 0, max = iter, style = 3)#
			                                                                                                          # 2. set up matrices for parameters which will be saved#
#
					map =    matrix(0,nrow = iter, ncol = 1)#
#
					Loglike =   matrix(0,nrow = iter, ncol = 1)#
#
					Bigmu = replicate(nCh,  matrix(0,nrow = iter, ncol = k)	, simplify=F)#
#
					Bigsigma=replicate(nCh,  matrix(0,nrow = iter, ncol = k)	, simplify=F)#
#
					Bigp =  replicate(nCh,  matrix(0,nrow = iter, ncol = k)	, simplify=F)#
#
					Pzs =   replicate(nCh,  matrix(0,nrow = n, ncol = k)	, simplify=F)#
#
					ZSaved=	replicate(nCh,  matrix(0,nrow = n, ncol = iter)	, simplify=F)#
#
					SteadyScore<-data.frame("Iteration"=c(1:iter), "K0"=k)#
			                                                                                                          # start chains and  create inits needed for i=1#
#
					for (.ch in 1:nCh){#
#
					Bigmu[[.ch]][1,] <- mux$mu                                                                        # initial value of mu's#
#
					mu0=mux$mu#
#
					Bigp[[.ch]][1,] = mux$p                                                                           # inital value of p's#
#
					p0=mux$p#
#
					Bigsigma[[.ch]][1,] = mux$sigma                                                                   # inits for sigma#
#
					sig0=mux$sigma	}#
			                                                                                                          #  		initialize chains:  ie. iteration 1:#
#
					j<-1
for (.ch in 1:nCh){#
					for (i in 1:n) {#
					Pzs[[.ch]][i,]<-(p0/sqrt(sig0))*exp(-((Y[i]-mu0)^2)/(2*sig0))#
#
					Pzs[[.ch]][i,]<-Pzs[[.ch]][i,]/sum(Pzs[[.ch]][i,]) }}											  # Scale to equal 1?#
#
			                                                                                                          # 2 Make indicator matrix of assignments based on Pzs#
			                                                                                                          #	sample 1 of the k classes for each row by Pzs (prob)#
					for (.ch in 1:nCh){#
					Z<-matrix()#
					for (i in 1:n){Z[i]=sample((1:k),1, prob=Pzs[[.ch]][i,])}#
					matk = matrix((1:k), nrow = n, ncol = k, byrow = T)#
					IndiZ = (Z == matk)#
					ZSaved[[.ch]][,1]<-Z#
			                                                                                                          # 3 compute ns and sx#
					ns = apply(IndiZ,2,sum)#
					for (i in 1:length(ns)){ if ( is.na(ns[i])) ns[i]<-0 }#
					sx = apply(IndiZ*Y, 2, sum)#
#
			                                                                                                          # 4 Generate P[j](t) from dirichlet  (and save)#
					Bigp[[.ch]][j,] = rdirichlet(m=1,par= ns+alphas[.ch])#
#
			                                                                                                          # 5	Generate Mu's   (and save)#
					Bigmu[[.ch]][j,]<-rnorm(k,	mean=(lambda*tau+sx)/(tau+ns), sd=sqrt(Bigsigma[[.ch]][1,]/(tau+ns))) # must be sqrt as r takes in sd not var#
#
			                                                                                                          #	ybar<-sx/ns#
					for (i in 1:length(Bigmu[[.ch]][j,])){ if ( is.na(Bigmu[[.ch]][j,i])) Bigmu[[.ch]][j,i]<-0 }#
			                                                                                                          # 6  Compute sv[j](t)#
					.bmu<- matrix((1:k), nrow = n, ncol = k, byrow = T)#
					for (t in 1:n) {.bmu[t,]<-Bigmu[[.ch]][j,]}#
					sv<-apply((Y*IndiZ-.bmu*IndiZ)^2, 2, sum)                                                         # changes, added /ns#
#
			                                                                                                          # 7 Generate Sigma's (and save)#
#
					Bigsigma[[.ch]][j,]<- rinvgamma(k, a+(ns+1)/2,	b+0.5*tau*(Bigmu[[.ch]][j,]-lambda)^2+0.5*sv)#
#
					}#
#
			                                                                                                          # Log Likelihood: # Sum-n (log Sum-K ( weights x dnorm (y,thetas)))
# Log Likelihood: # Sum-n (log Sum-K ( weights x dnorm (y,thetas)))#
					for (i in 1:n){#
					non0id<-c(1:k)[ns > 0]#
					Loglike[j]<-Loglike[j]+ log(#
						 sum( Bigp[[nCh]][j,non0id]*dnorm(Y[i], mean=Bigmu[[nCh]][j,non0id], sd=sqrt(Bigsigma[[nCh]][j,non0id]))))}
ZSaved
as.matrix(ZSaved[[nCh]][,j-1], nrow=2)
matrix(ZSaved[[nCh]][,j-1], nrow=2)
matrix(ZSaved[[nCh]], nrow=2)
matrix(ZSaved[[nCh]])
as.matrix(ZSaved[[nCh]])
ZSaved[[nCh]]
ZSaved[[nCh]][,j]
as.matrix(ZSaved[[nCh]][,j])
as.matrix(ZSaved[[nCh]][,j], nrow=2)
as.matrix(ZSaved[[nCh]][,j], ncol=2)
matrix(ZSaved[[nCh]][,j], ncol=2)
c(matrix(ZSaved[[nCh]][,j], ncol=2))
ZSaved[[nCh]][,j]
c(matrix(ZSaved[[nCh]][,j], ncol=2))
matrix(ZSaved[[nCh]][,j], ncol=2)
image(matrix(ZSaved[[nCh]][,j], ncol=2))
Bigmu[[1]]
Bigmu[[1]][,1]
Bigmu[[1]][1,]
order(Bigmu[[1]][1,])
grey(seq(0,1,by=1/K)[order(Bigmu[[1]][1,])]
grey(seq(0,1,by=1/K))[order(Bigmu[[1]][1,])]
grey(seq(0,1,by=1/10))[order(Bigmu[[1]][1,])]
grey(seq(0,1,by=1/K))[order(Bigmu[[1]][1,])]
order(Bigmu[[1]][1,])
Bigmu[[1]][1,]
seq(0,1,by=1/K)
seq(0,1,by=1/10)
seq(0,1,by=1/10)[order(Bigmu[[1]][1,])]
?order
Y
I(10)
diag(10)
c(diag(10))
c(matrix(diag(10)))
n<-10#
#
Funk<-function(x){x+n}#
#
Funk(1)
?sample
library("mvtnorm")#
library("dplyr")#
library("compiler")#
library("microbenchmark")#
library("ggplot2")#
library("mvnfast")#
#
Zmix_main<-function(#
          y,#
          iterations=2000,#
          k=10,#
          alphas= c(1/2^(c(6, 10, 30))),#
          burn=500,#
          init.method="Kmeans",#
          verbose=TRUE#
          ){#
#
      ## INNER FUNCTIONS#
      # Initial clustering#
      initiate_Z<-function(x, method=c("Kmeans", "random", "single") ){#
        if(method=="Kmeans"){#
        init.allocations<-  as.vector(kmeans(x, centers=k)$cluster)#
        } else if (method=="random"){#
          init.allocations<-  base::sample(c(1:k), n, replace=TRUE)#
        } else if (method=="single"){#
          init.allocations<-  rep(1, n)#
        }#
        return(init.allocations)#
      }#
      initiate_values<-function(ZZ){#
        IndiZ <-  (ZZ == matrix((1:k), nrow = n, ncol = k, byrow = T))#
        ns <-     apply(IndiZ,2,sum)  	#  size of each group#
        .Ysplit<- replicate(k, list())	#storage to group Y's#
        WkZ<-	    replicate(k, list(0))	#storage for within group variability#
        ybar<-	  replicate(k, list(0))#
        for (.i in 1:k){#
          .Ysplit[[.i]]<-y[ZZ==.i,]#
          if (ns[.i]>1){					# for groups with >1 obsevations#
            ybar[[.i]]<- as.matrix(t(apply(.Ysplit[[.i]], 2, mean) ))#
            } else if (ns[.i]==1){#
              ybar[[.i]]<-	t( as.matrix(.Ysplit[[.i]]))#
              } else {#
                ybar[[.i]]<-	NA#
              }#
          #Within group unexplained variability#
          if (ns[.i]==0) {#
            WkZ[[.i]]<-	NA#
            } else if (ns[.i]==1){#
              WkZ[[.i]]<-crossprod(as.matrix(.Ysplit[[.i]]-ybar[[.i]]))#
              } else {#
                for ( .n in 1:ns[.i]){#
                  WkZ[[.i]]<-WkZ[[.i]]+ crossprod( .Ysplit[[.i]][.n,]-ybar[[.i]])#
                }#
              }#
            }#
            list('ns'=ns,'ybar'=ybar, 'WkZ'=WkZ)#
        }#
      MuSample<-function(covs, ns_mu, WkZ_mu, ybar_mu){#
          # covs<-matrix(covs, nrow=r, byrow=T)#
          if (ns_mu==0) {#
            rmvn(1, b0, covs/n0)#
            } else {#
              bk<-(n0/(ns_mu+n0))*b0+(ns_mu/(n0+ns_mu))*ybar_mu#
              Bk<-(1/(ns_mu+n0))*covs#
              rmvn(1, t(bk), Bk)#
            }#
        }#
      Update_probs<-function(Mu, Cov, Pi ){#
          update_group_Prob <-function(x){#
              a1<-dmvn(y, Mu[[x]], Cov[[x]])#
              a2<-a1*Pi[x]#
              # for (i in 1:n)		{#
              #   if (sum(PZs[i,])==0) {#
              #     PZs[i,]<-rep(1/k,k)    # if all probs are zero, randomly allocate obs. very rare, might lead to crap results#
              #   }}#
              return(a2)#
            }#
          # apply to each component#
          ugp<-mapply(update_group_Prob, c(1:k))#
          # scale each row#
          ugp/apply(ugp, 1, sum)#
        }#
      CovSample<-function(ns_cov, WkZ_cov, ybar_cov){#
          c_cov<-c0+ns_cov#
          if (ns_cov==0) {#
            MCMCpack::riwish(c0, C0)#
          } else {#
            C_cov<- C0 +((ns_cov*n0)/(ns_cov+n0)*crossprod(ybar_cov-b0)) +WkZ_cov#
            MCMCpack::riwish(c_cov,C_cov)#
          }#
        }#
      Update_Zs<-function(PZs){mapply(function(x) sample( (1:k), 1, prob=PZs[x,]), c(1:n))}#
      parallelAccept<-function(w1, w2, a1, a2){#
          w1[w1< 1e-200]<-1e-200   # truncate so super small values dont crash everyting#
          w2[w2< 1e-200]<-1e-200#
            T1<-dDirichlet(w2, a1, log=TRUE)#
            T2<-dDirichlet(w1, a2, log=TRUE)#
            B1<-dDirichlet(w1, a1, log=TRUE)#
            B2<-dDirichlet(w2, a2, log=TRUE)#
            MH<-min(1,	exp(T1+T2-B1-B2))#
          Ax<-sample(c(1,0), 1, prob=c(MH,1-MH))#
          return(Ax)}#
      dDirichlet<-function (x, alpha, log = FALSE) {#
          dlog = lgamma(sum(alpha)) + sum((alpha - 1) * log(x)) - sum(lgamma(alpha))#
          result = ifelse(!log, exp(dlog), dlog)#
          return(result)#
      	}#
      # function#
 rdirichlet<-cmpfun(rdirichlet)#
#
## compute basic values#
  nCh<- length(alphas)#
  r<-   dim(y)[2]#
  n<-   dim(y)[1]#
  Loglike <-   rep(0,iterations)#
  SteadyScore<-data.frame("Iteration"=c(1:iterations), "K0"=0)#
#
# hyperpriors#
  n0<-  tau <-1   # this is tau equiv#
  Ck<-	replicate(k, list())#
  b0<-  apply(y,2,mean)#
  c0<-  r+1#
  C0<-  0.75*cov(y)#
  d<-   sum(c(1:r))+r#
#
## parameters to estimate#
## storage structure  par[[chain]][[iterations]][[k]]#
  Ps<-     replicate(nCh, replicate(iterations, list()))#
  Mus<-    replicate(nCh, replicate(iterations, list()))#
  Covs<-   replicate(nCh, replicate(iterations, list()))#
  Zs<-     replicate(nCh, replicate(iterations, list()))#
#
  if(verbose==TRUE){pb <- txtProgressBar(min = 0, max = iterations, style = 3)}#
#
# FOR EACH ITERATION#
for (.it in 1:iterations){  #for each iteration#
  # TRACKER#
if(verbose==TRUE && .it %% 10 == 0) {#
  Sys.sleep(0.01)#
  par(mfrow=c(2,1))#
  plot(SteadyScore$K0~SteadyScore$Iteration, main='#non-empty groups', type='l')#
  ts.plot( sapply(Ps[[nCh]], cbind), main='Target Weights', col=rainbow(k))#
  Sys.sleep(0)#
  setTxtProgressBar(pb, .it)#
}#
#
  for (.ch in 1:nCh){   #for each chain#
#
# Input values#
    if (.it==1){#
      # iteration=1, initiallize input values#
      init.Z<-  initiate_Z(y, init.method)#
      input.values<-    initiate_values(init.Z) # compute values needed#
        ns<-  input.values$ns#
        ybar<-input.values$ybar#
        WkZ<- input.values$WkZ#
      } else {#
      # Update given current pars#
      input.values<-initiate_values(Zs[[.ch]][[.it-1]])  # check me#
        ns<-    input.values$ns                         # check me#
        ybar<-  input.values$ybar                       # check me#
        WkZ<-   input.values$WkZ                        # check me#
      }#
#
      # STEP 2.1 : GENERATE Samples for WEIGHTS from DIRICHLET dist#
      Ps[[.ch]][[.it]] <- rdirichlet(m=1, par= ns+alphas[.ch])#
#
      # STEP 2.2 GENERATE Samples from Covariance#
      Covs[[.ch]][[.it]]<-mapply(CovSample, ns, WkZ, ybar, SIMPLIFY=FALSE)#
#
      # STEP 2.3 GENERATE SAMPLEs of Means#
      Mus[[.ch]][[.it]]<- mapply(MuSample,Covs[[.ch]][[.it]], ns, WkZ, ybar, SIMPLIFY=FALSE)#
#
      # STEP 3.1: Draw new classification probabilities:#
      PZs<-Update_probs(Mus[[.ch]][[.it]], Covs[[.ch]][[.it]], Ps[[.ch]][[.it]])#
#
      # STEP 3.2: Update allocations based on probabilitie#
      Zs[[.ch]][[.it]]<-Update_Zs(PZs)#
#
} # end chain loop#
## PRIOR PARALLEL TEMPERING#
if(.it>20 && runif(1)<.9){#
  Chain1<-sample( 1:(nCh-1), 1) ; 	Chain2<-Chain1+1#
  MHratio<- parallelAccept(Ps[[Chain1]][[.it]], Ps[[Chain2]][[.it]], rep(alphas[Chain1],k), rep(alphas[Chain2],k))#
  if (MHratio==1){#
  # Flip the allocations#
  .z1<-	Zs[[Chain1]][[.it]] 	;.z2<-	Zs[[Chain2]][[.it]]#
  Zs[[Chain1]][[.it]]<-.z2 	;Zs[[Chain2]][[.it]]<-.z1#
#
  #Mu#
  .mu1<-	Mus[[Chain1]][[.it]] 	;.mu2<-	Mus[[Chain2]][[.it]]#
  Mus[[Chain1]][[.it]]<-.mu2 	;Mus[[Chain2]][[.it]]<-.mu1#
#
  #Cov#
  .cv1<-	Covs[[Chain1]][[.it]] 	;.cv2<-	Covs[[Chain2]][[.it]]#
  Covs[[Chain1]][[.it]]<-.cv2 	;Covs[[Chain2]][[.it]]<-.cv1#
#
  #Ps#
  .p1<-	Ps[[Chain1]][[.it]] 	;.p2<-	Ps[[Chain2]][[.it]]#
  Ps[[Chain1]][[.it]]<-.p2 	;Ps[[Chain2]][[.it]]<-.p1#
}#
}#
#
# count number of occupied components#
SteadyScore$K0[.it]<- factor(Zs[[.ch]][[.it]]) %>% levels(.) %>% length(.)#
#
# compute Log Likelihood ( still need to optomize)#
for (i in 1:n){#
  non0id<-Zs[[nCh]][[.it]] %>% factor() %>% levels() %>% as.numeric()#
    .ll<-0#
    for (numK in 1:length(non0id)){#
       .ll<-.ll+#
       Ps[[nCh]][[.it]][non0id[numK]]*dmvn(#
         y[i,], Mus[[nCh]][[.it]][[ non0id[numK] ]],#
         Covs[[nCh]][[.it]][[ non0id[numK] ]])#
    }#
  Loglike[.it]<-Loglike[.it]+ log(.ll)#
}#
} # end iteration loop#
if(verbose==TRUE){close(pb)}#
#
  burn<-burn+1#
	nCh                                     #number of chains#
	Mu.burned<-Mus[[nCh]][[burn:iterations]]#
	Cov.burned<-Covs[[nCh]][[burn:iterations]]#
	Ps.burned<-Ps[[nCh]][[burn:iterations]]#
	Zs.burned<-Zs[[nCh]][[burn:iterations]]#
  Loglike.burned<-Loglike[burn:iterations]#
	SteadyScore.burned<-SteadyScore$K0[burn:iterations]#
#
	return(list(#
    Mu = Mu.burned,#
    Cov=Cov.burned,#
    Ps= Ps.burned,#
    Zs=Zs.burned,#
    k.occupied=SteadyScore,#
    Log.likelihood=Loglike, y=y))#
}
rdirichlet<-function(m=1,par){#
			k=length(par)         # for two mixture model k=2#
			mat=matrix(0,m,k)       # makes an empty 1x2 matrix to store values#
			for (i in 1:m)          # at this stage m is 1, when is it not 1?#
			{#
			sim=rgamma(k,shape=par,scale=1)#
			mat[i,]=sim/sum(sim)      # simulated gamma scaled to sum to 1 and stored in matrix #
			}#
			mat               # matrix outputed#
			}
SimMVN<-function(Means, Covs, N, P){#
			TrueK<-length(P)#
			r<-length(Means[[1]])#
			Ys<-matrix(0,nrow = N, ncol = r)#
			Zs<-matrix(0,nrow = N, ncol = 1)#
			for (.n in 1:N){#
			#draw Z according to probability#
			.z<-sample(c(1:TrueK), size=1, prob=P)#
			Zs[.n,]<-.z#
			#generate random sample#
			Ys[.n,]<-rmvnorm(1, Means[[.z]], Covs[[.z]])#
			}#
			list(Y=Ys, Z=Zs)}#
    S3<-list(     "Means"=list( c(3,5), c(1,5)),#
			"Covs"= list( matrix(c(10,0.1,0.1,0.5), nrow=2),#
			matrix(c(0.5,0.1,0.1,10), nrow=2)),#
			"P"=c(0.7,0.3))#
#
	y<-SimMVN(Means=S3$Means, Covs=S3$Covs, N=50, P=S3$P)$Y
y
test1<-Zmix_main(y,#
          iterations=20,#
          k=5,#
          alphas= c(1/2^(c(6, 10, 30))),#
          burn=10,#
          init.method="Kmeans"#
          )
library("mvtnorm")#
library("dplyr")#
library("compiler")#
library("microbenchmark")#
library("ggplot2")#
library("mvnfast")#
#
Zmix_main<-function(#
          y,#
          iterations=2000,#
          k=10,#
          alphas= c(1/2^(c(6, 10, 30))),#
          burn=500,#
          init.method="Kmeans",#
          verbose=TRUE#
          ){#
#
      ## INNER FUNCTIONS#
      # Initial clustering#
      initiate_Z<-function(x, method=c("Kmeans", "random", "single") ){#
        if(method=="Kmeans"){#
        init.allocations<-  as.vector(kmeans(x, centers=k)$cluster)#
        } else if (method=="random"){#
          init.allocations<-  base::sample(c(1:k), n, replace=TRUE)#
        } else if (method=="single"){#
          init.allocations<-  rep(1, n)#
        }#
        return(init.allocations)#
      }#
      initiate_values<-function(ZZ){#
        IndiZ <-  (ZZ == matrix((1:k), nrow = n, ncol = k, byrow = T))#
        ns <-     apply(IndiZ,2,sum)  	#  size of each group#
        .Ysplit<- replicate(k, list())	#storage to group Y's#
        WkZ<-	    replicate(k, list(0))	#storage for within group variability#
        ybar<-	  replicate(k, list(0))#
        for (.i in 1:k){#
          .Ysplit[[.i]]<-y[ZZ==.i,]#
          if (ns[.i]>1){					# for groups with >1 obsevations#
            ybar[[.i]]<- as.matrix(t(apply(.Ysplit[[.i]], 2, mean) ))#
            } else if (ns[.i]==1){#
              ybar[[.i]]<-	t( as.matrix(.Ysplit[[.i]]))#
              } else {#
                ybar[[.i]]<-	NA#
              }#
          #Within group unexplained variability#
          if (ns[.i]==0) {#
            WkZ[[.i]]<-	NA#
            } else if (ns[.i]==1){#
              WkZ[[.i]]<-crossprod(as.matrix(.Ysplit[[.i]]-ybar[[.i]]))#
              } else {#
                for ( .n in 1:ns[.i]){#
                  WkZ[[.i]]<-WkZ[[.i]]+ crossprod( .Ysplit[[.i]][.n,]-ybar[[.i]])#
                }#
              }#
            }#
            list('ns'=ns,'ybar'=ybar, 'WkZ'=WkZ)#
        }#
      MuSample<-function(covs, ns_mu, WkZ_mu, ybar_mu){#
          # covs<-matrix(covs, nrow=r, byrow=T)#
          if (ns_mu==0) {#
            rmvn(1, b0, covs/n0)#
            } else {#
              bk<-(n0/(ns_mu+n0))*b0+(ns_mu/(n0+ns_mu))*ybar_mu#
              Bk<-(1/(ns_mu+n0))*covs#
              rmvn(1, t(bk), Bk)#
            }#
        }#
      Update_probs<-function(Mu, Cov, Pi ){#
          update_group_Prob <-function(x){#
              a1<-dmvn(y, Mu[[x]], Cov[[x]])#
              a2<-a1*Pi[x]#
              # for (i in 1:n)		{#
              #   if (sum(PZs[i,])==0) {#
              #     PZs[i,]<-rep(1/k,k)    # if all probs are zero, randomly allocate obs. very rare, might lead to crap results#
              #   }}#
              return(a2)#
            }#
          # apply to each component#
          ugp<-mapply(update_group_Prob, c(1:k))#
          # scale each row#
          ugp/apply(ugp, 1, sum)#
        }#
      CovSample<-function(ns_cov, WkZ_cov, ybar_cov){#
          c_cov<-c0+ns_cov#
          if (ns_cov==0) {#
            MCMCpack::riwish(c0, C0)#
          } else {#
            C_cov<- C0 +((ns_cov*n0)/(ns_cov+n0)*crossprod(ybar_cov-b0)) +WkZ_cov#
            MCMCpack::riwish(c_cov,C_cov)#
          }#
        }#
      Update_Zs<-function(PZs){mapply(function(x) sample( (1:k), 1, prob=PZs[x,]), c(1:n))}#
      parallelAccept<-function(w1, w2, a1, a2){#
          w1[w1< 1e-200]<-1e-200   # truncate so super small values dont crash everyting#
          w2[w2< 1e-200]<-1e-200#
            T1<-dDirichlet(w2, a1, log=TRUE)#
            T2<-dDirichlet(w1, a2, log=TRUE)#
            B1<-dDirichlet(w1, a1, log=TRUE)#
            B2<-dDirichlet(w2, a2, log=TRUE)#
            MH<-min(1,	exp(T1+T2-B1-B2))#
          Ax<-sample(c(1,0), 1, prob=c(MH,1-MH))#
          return(Ax)}#
      dDirichlet<-function (x, alpha, log = FALSE) {#
          dlog = lgamma(sum(alpha)) + sum((alpha - 1) * log(x)) - sum(lgamma(alpha))#
          result = ifelse(!log, exp(dlog), dlog)#
          return(result)#
      	}#
      # function#
 rdirichlet<-cmpfun(rdirichlet)#
#
## compute basic values#
  nCh<- length(alphas)#
  r<-   dim(y)[2]#
  n<-   dim(y)[1]#
  Loglike <-   rep(0,iterations)#
  SteadyScore<-data.frame("Iteration"=c(1:iterations), "K0"=0)#
#
# hyperpriors#
  n0<-  tau <-1   # this is tau equiv#
  Ck<-	replicate(k, list())#
  b0<-  apply(y,2,mean)#
  c0<-  r+1#
  C0<-  0.75*cov(y)#
  d<-   sum(c(1:r))+r#
#
## parameters to estimate#
## storage structure  par[[chain]][[iterations]][[k]]#
  Ps<-     replicate(nCh, replicate(iterations, list()))#
  Mus<-    replicate(nCh, replicate(iterations, list()))#
  Covs<-   replicate(nCh, replicate(iterations, list()))#
  Zs<-     replicate(nCh, replicate(iterations, list()))#
#
  if(verbose==TRUE){pb <- txtProgressBar(min = 0, max = iterations, style = 3)}#
#
# FOR EACH ITERATION#
for (.it in 1:iterations){  #for each iteration#
  # TRACKER#
if(verbose==TRUE && .it %% 10 == 0) {#
  Sys.sleep(0.01)#
  par(mfrow=c(2,1))#
  plot(SteadyScore$K0~SteadyScore$Iteration, main='#non-empty groups', type='l')#
  ts.plot( sapply(Ps[[nCh]], cbind), main='Target Weights', col=rainbow(k))#
  Sys.sleep(0)#
  setTxtProgressBar(pb, .it)#
}#
#
  for (.ch in 1:nCh){   #for each chain#
#
# Input values#
    if (.it==1){#
      # iteration=1, initiallize input values#
      init.Z<-  initiate_Z(y, init.method)#
      input.values<-    initiate_values(init.Z) # compute values needed#
        ns<-  input.values$ns#
        ybar<-input.values$ybar#
        WkZ<- input.values$WkZ#
      } else {#
      # Update given current pars#
      input.values<-initiate_values(Zs[[.ch]][[.it-1]])  # check me#
        ns<-    input.values$ns                         # check me#
        ybar<-  input.values$ybar                       # check me#
        WkZ<-   input.values$WkZ                        # check me#
      }#
#
      # STEP 2.1 : GENERATE Samples for WEIGHTS from DIRICHLET dist#
      Ps[[.ch]][[.it]] <- rdirichlet(m=1, par= ns+alphas[.ch])#
#
      # STEP 2.2 GENERATE Samples from Covariance#
      Covs[[.ch]][[.it]]<-mapply(CovSample, ns, WkZ, ybar, SIMPLIFY=FALSE)#
#
      # STEP 2.3 GENERATE SAMPLEs of Means#
      Mus[[.ch]][[.it]]<- mapply(MuSample,Covs[[.ch]][[.it]], ns, WkZ, ybar, SIMPLIFY=FALSE)#
#
      # STEP 3.1: Draw new classification probabilities:#
      PZs<-Update_probs(Mus[[.ch]][[.it]], Covs[[.ch]][[.it]], Ps[[.ch]][[.it]])#
#
      # STEP 3.2: Update allocations based on probabilitie#
      Zs[[.ch]][[.it]]<-Update_Zs(PZs)#
#
} # end chain loop#
## PRIOR PARALLEL TEMPERING#
if(.it>20 && runif(1)<.9){#
  Chain1<-sample( 1:(nCh-1), 1) ; 	Chain2<-Chain1+1#
  MHratio<- parallelAccept(Ps[[Chain1]][[.it]], Ps[[Chain2]][[.it]], rep(alphas[Chain1],k), rep(alphas[Chain2],k))#
  if (MHratio==1){#
  # Flip the allocations#
  .z1<-	Zs[[Chain1]][[.it]] 	;.z2<-	Zs[[Chain2]][[.it]]#
  Zs[[Chain1]][[.it]]<-.z2 	;Zs[[Chain2]][[.it]]<-.z1#
#
  #Mu#
  .mu1<-	Mus[[Chain1]][[.it]] 	;.mu2<-	Mus[[Chain2]][[.it]]#
  Mus[[Chain1]][[.it]]<-.mu2 	;Mus[[Chain2]][[.it]]<-.mu1#
#
  #Cov#
  .cv1<-	Covs[[Chain1]][[.it]] 	;.cv2<-	Covs[[Chain2]][[.it]]#
  Covs[[Chain1]][[.it]]<-.cv2 	;Covs[[Chain2]][[.it]]<-.cv1#
#
  #Ps#
  .p1<-	Ps[[Chain1]][[.it]] 	;.p2<-	Ps[[Chain2]][[.it]]#
  Ps[[Chain1]][[.it]]<-.p2 	;Ps[[Chain2]][[.it]]<-.p1#
}#
}#
#
# count number of occupied components#
SteadyScore$K0[.it]<- factor(Zs[[.ch]][[.it]]) %>% levels(.) %>% length(.)#
#
# compute Log Likelihood ( still need to optomize)#
for (i in 1:n){#
  non0id<-Zs[[nCh]][[.it]] %>% factor() %>% levels() %>% as.numeric()#
    .ll<-0#
    for (numK in 1:length(non0id)){#
       .ll<-.ll+#
       Ps[[nCh]][[.it]][non0id[numK]]*dmvn(#
         y[i,], Mus[[nCh]][[.it]][[ non0id[numK] ]],#
         Covs[[nCh]][[.it]][[ non0id[numK] ]])#
    }#
  Loglike[.it]<-Loglike[.it]+ log(.ll)#
}#
} # end iteration loop#
if(verbose==TRUE){close(pb)}#
#
  burn<-burn+1#
	nCh                                     #number of chains#
	Mu.burned<-Mus[[nCh]][c(burn:iterations)]#
	Cov.burned<-Covs[[nCh]][burn:iterations]#
	Ps.burned<-Ps[[nCh]][burn:iterations]#
	Zs.burned<-Zs[[nCh]][burn:iterations]#
  Loglike.burned<-Loglike[burn:iterations]#
	SteadyScore.burned<-SteadyScore$K0[burn:iterations]#
#
	return(list(#
    Mu = Mu.burned,#
    Cov=Cov.burned,#
    Ps= Ps.burned,#
    Zs=Zs.burned,#
    k.occupied=SteadyScore,#
    Log.likelihood=Loglike, y=y))#
}
test1<-Zmix_main(y,#
          iterations=50,#
          k=5,#
          alphas= c(1/2^(c(6, 10, 30))),#
          burn=20,#
          init.method="Kmeans"#
          )
test1
library("mvtnorm")#
library("dplyr")#
library("compiler")#
library("microbenchmark")#
library("ggplot2")#
library("mvnfast")#
#
Zmix_main<-function(#
          y,#
          iterations=2000,#
          k=10,#
          alphas= c(1/2^(c(6, 10, 30))),#
          burn=500,#
          init.method="Kmeans",#
          verbose=TRUE#
          ){#
#
      ## INNER FUNCTIONS#
      # Initial clustering#
      initiate_Z<-function(x, method=c("Kmeans", "random", "single") ){#
        if(method=="Kmeans"){#
        init.allocations<-  as.vector(kmeans(x, centers=k)$cluster)#
        } else if (method=="random"){#
          init.allocations<-  base::sample(c(1:k), n, replace=TRUE)#
        } else if (method=="single"){#
          init.allocations<-  rep(1, n)#
        }#
        return(init.allocations)#
      }#
      initiate_values<-function(ZZ){#
        IndiZ <-  (ZZ == matrix((1:k), nrow = n, ncol = k, byrow = T))#
        ns <-     apply(IndiZ,2,sum)  	#  size of each group#
        .Ysplit<- replicate(k, list())	#storage to group Y's#
        WkZ<-	    replicate(k, list(0))	#storage for within group variability#
        ybar<-	  replicate(k, list(0))#
        for (.i in 1:k){#
          .Ysplit[[.i]]<-y[ZZ==.i,]#
          if (ns[.i]>1){					# for groups with >1 obsevations#
            ybar[[.i]]<- as.matrix(t(apply(.Ysplit[[.i]], 2, mean) ))#
            } else if (ns[.i]==1){#
              ybar[[.i]]<-	t( as.matrix(.Ysplit[[.i]]))#
              } else {#
                ybar[[.i]]<-	NA#
              }#
          #Within group unexplained variability#
          if (ns[.i]==0) {#
            WkZ[[.i]]<-	NA#
            } else if (ns[.i]==1){#
              WkZ[[.i]]<-crossprod(as.matrix(.Ysplit[[.i]]-ybar[[.i]]))#
              } else {#
                for ( .n in 1:ns[.i]){#
                  WkZ[[.i]]<-WkZ[[.i]]+ crossprod( .Ysplit[[.i]][.n,]-ybar[[.i]])#
                }#
              }#
            }#
            list('ns'=ns,'ybar'=ybar, 'WkZ'=WkZ)#
        }#
      MuSample<-function(covs, ns_mu, WkZ_mu, ybar_mu){#
          # covs<-matrix(covs, nrow=r, byrow=T)#
          if (ns_mu==0) {#
            rmvn(1, b0, covs/n0)#
            } else {#
              bk<-(n0/(ns_mu+n0))*b0+(ns_mu/(n0+ns_mu))*ybar_mu#
              Bk<-(1/(ns_mu+n0))*covs#
              rmvn(1, t(bk), Bk)#
            }#
        }#
      Update_probs<-function(Mu, Cov, Pi ){#
          update_group_Prob <-function(x){#
              a1<-dmvn(y, Mu[[x]], Cov[[x]])#
              a2<-a1*Pi[x]#
              # for (i in 1:n)		{#
              #   if (sum(PZs[i,])==0) {#
              #     PZs[i,]<-rep(1/k,k)    # if all probs are zero, randomly allocate obs. very rare, might lead to crap results#
              #   }}#
              return(a2)#
            }#
          # apply to each component#
          ugp<-mapply(update_group_Prob, c(1:k))#
          # scale each row#
          ugp/apply(ugp, 1, sum)#
        }#
      CovSample<-function(ns_cov, WkZ_cov, ybar_cov){#
          c_cov<-c0+ns_cov#
          if (ns_cov==0) {#
            MCMCpack::riwish(c0, C0)#
          } else {#
            C_cov<- C0 +((ns_cov*n0)/(ns_cov+n0)*crossprod(ybar_cov-b0)) +WkZ_cov#
            MCMCpack::riwish(c_cov,C_cov)#
          }#
        }#
      Update_Zs<-function(PZs){mapply(function(x) sample( (1:k), 1, prob=PZs[x,]), c(1:n))}#
      parallelAccept<-function(w1, w2, a1, a2){#
          w1[w1< 1e-200]<-1e-200   # truncate so super small values dont crash everyting#
          w2[w2< 1e-200]<-1e-200#
            T1<-dDirichlet(w2, a1, log=TRUE)#
            T2<-dDirichlet(w1, a2, log=TRUE)#
            B1<-dDirichlet(w1, a1, log=TRUE)#
            B2<-dDirichlet(w2, a2, log=TRUE)#
            MH<-min(1,	exp(T1+T2-B1-B2))#
          Ax<-sample(c(1,0), 1, prob=c(MH,1-MH))#
          return(Ax)}#
      dDirichlet<-function (x, alpha, log = FALSE) {#
          dlog = lgamma(sum(alpha)) + sum((alpha - 1) * log(x)) - sum(lgamma(alpha))#
          result = ifelse(!log, exp(dlog), dlog)#
          return(result)#
      	}#
      # function#
 rdirichlet<-cmpfun(rdirichlet)#
#
## compute basic values#
  nCh<- length(alphas)#
  r<-   dim(y)[2]#
  n<-   dim(y)[1]#
  Loglike <-   rep(0,iterations)#
  SteadyScore<-data.frame("Iteration"=c(1:iterations), "K0"=0)#
#
# hyperpriors#
  n0<-  tau <-1   # this is tau equiv#
  Ck<-	replicate(k, list())#
  b0<-  apply(y,2,mean)#
  c0<-  r+1#
  C0<-  0.75*cov(y)#
  d<-   sum(c(1:r))+r#
#
## parameters to estimate#
## storage structure  par[[chain]][[iterations]][[k]]#
  Ps<-     replicate(nCh, replicate(iterations, list()))#
  Mus<-    replicate(nCh, replicate(iterations, list()))#
  Covs<-   replicate(nCh, replicate(iterations, list()))#
  Zs<-     replicate(nCh, replicate(iterations, list()))#
#
  if(verbose==TRUE){pb <- txtProgressBar(min = 0, max = iterations, style = 3)}#
#
# FOR EACH ITERATION#
for (.it in 1:iterations){  #for each iteration#
  # TRACKER#
if(verbose==TRUE && .it %% 10 == 0) {#
  Sys.sleep(0.01)#
  par(mfrow=c(2,1))#
  plot(SteadyScore$K0~SteadyScore$Iteration, main='#non-empty groups', type='l')#
  ts.plot( t(sapply(Ps[[nCh]], rbind)), main='Target Weights', col=rainbow(k))#
  Sys.sleep(0)#
  setTxtProgressBar(pb, .it)#
}#
#
  for (.ch in 1:nCh){   #for each chain#
#
# Input values#
    if (.it==1){#
      # iteration=1, initiallize input values#
      init.Z<-  initiate_Z(y, init.method)#
      input.values<-    initiate_values(init.Z) # compute values needed#
        ns<-  input.values$ns#
        ybar<-input.values$ybar#
        WkZ<- input.values$WkZ#
      } else {#
      # Update given current pars#
      input.values<-initiate_values(Zs[[.ch]][[.it-1]])  # check me#
        ns<-    input.values$ns                         # check me#
        ybar<-  input.values$ybar                       # check me#
        WkZ<-   input.values$WkZ                        # check me#
      }#
#
      # STEP 2.1 : GENERATE Samples for WEIGHTS from DIRICHLET dist#
      Ps[[.ch]][[.it]] <- rdirichlet(m=1, par= ns+alphas[.ch])#
#
      # STEP 2.2 GENERATE Samples from Covariance#
      Covs[[.ch]][[.it]]<-mapply(CovSample, ns, WkZ, ybar, SIMPLIFY=FALSE)#
#
      # STEP 2.3 GENERATE SAMPLEs of Means#
      Mus[[.ch]][[.it]]<- mapply(MuSample,Covs[[.ch]][[.it]], ns, WkZ, ybar, SIMPLIFY=FALSE)#
#
      # STEP 3.1: Draw new classification probabilities:#
      PZs<-Update_probs(Mus[[.ch]][[.it]], Covs[[.ch]][[.it]], Ps[[.ch]][[.it]])#
#
      # STEP 3.2: Update allocations based on probabilitie#
      Zs[[.ch]][[.it]]<-Update_Zs(PZs)#
#
} # end chain loop#
## PRIOR PARALLEL TEMPERING#
if(.it>20 && runif(1)<.9){#
  Chain1<-sample( 1:(nCh-1), 1) ; 	Chain2<-Chain1+1#
  MHratio<- parallelAccept(Ps[[Chain1]][[.it]], Ps[[Chain2]][[.it]], rep(alphas[Chain1],k), rep(alphas[Chain2],k))#
  if (MHratio==1){#
  # Flip the allocations#
  .z1<-	Zs[[Chain1]][[.it]] 	;.z2<-	Zs[[Chain2]][[.it]]#
  Zs[[Chain1]][[.it]]<-.z2 	;Zs[[Chain2]][[.it]]<-.z1#
#
  #Mu#
  .mu1<-	Mus[[Chain1]][[.it]] 	;.mu2<-	Mus[[Chain2]][[.it]]#
  Mus[[Chain1]][[.it]]<-.mu2 	;Mus[[Chain2]][[.it]]<-.mu1#
#
  #Cov#
  .cv1<-	Covs[[Chain1]][[.it]] 	;.cv2<-	Covs[[Chain2]][[.it]]#
  Covs[[Chain1]][[.it]]<-.cv2 	;Covs[[Chain2]][[.it]]<-.cv1#
#
  #Ps#
  .p1<-	Ps[[Chain1]][[.it]] 	;.p2<-	Ps[[Chain2]][[.it]]#
  Ps[[Chain1]][[.it]]<-.p2 	;Ps[[Chain2]][[.it]]<-.p1#
}#
}#
#
# count number of occupied components#
SteadyScore$K0[.it]<- factor(Zs[[.ch]][[.it]]) %>% levels(.) %>% length(.)#
#
# compute Log Likelihood ( still need to optomize)#
for (i in 1:n){#
  non0id<-Zs[[nCh]][[.it]] %>% factor() %>% levels() %>% as.numeric()#
    .ll<-0#
    for (numK in 1:length(non0id)){#
       .ll<-.ll+#
       Ps[[nCh]][[.it]][non0id[numK]]*dmvn(#
         y[i,], Mus[[nCh]][[.it]][[ non0id[numK] ]],#
         Covs[[nCh]][[.it]][[ non0id[numK] ]])#
    }#
  Loglike[.it]<-Loglike[.it]+ log(.ll)#
}#
} # end iteration loop#
if(verbose==TRUE){close(pb)}#
#
  burn<-burn+1#
	nCh                                     #number of chains#
	Mu.burned<-Mus[[nCh]][c(burn:iterations)]#
	Cov.burned<-Covs[[nCh]][burn:iterations]#
	Ps.burned<-Ps[[nCh]][burn:iterations]#
	Zs.burned<-Zs[[nCh]][burn:iterations]#
  Loglike.burned<-Loglike[burn:iterations]#
	SteadyScore.burned<-SteadyScore$K0[burn:iterations]#
#
	return(list(#
    Mu = Mu.burned,#
    Cov=Cov.burned,#
    Ps= Ps.burned,#
    Zs=Zs.burned,#
    k.occupied=SteadyScore,#
    Log.likelihood=Loglike, y=y))#
}
test1<-Zmix_main(y,#
          iterations=100,#
          k=5,#
          alphas= c(1/2^(c(6, 10, 30))),#
          burn=20,#
          init.method="Kmeans"#
          )
test1<-Zmix_main(y,#
          iterations=100,#
          k=5,#
          alphas= c(1/2^(c(6, 10, 30))),#
          burn=20,#
          init.method="single"#
          )
test1<-Zmix_main(y,#
          iterations=100,#
          k=5,#
          alphas= c(1/2^(c(6, 10, 30))),#
          burn=20,#
          init.method="random"#
          )
library("mvtnorm")#
library("dplyr")#
library("compiler")#
library("microbenchmark")#
library("ggplot2")#
library("mvnfast")#
#
Zmix_main<-function(#
          y,#
          iterations=2000,#
          k=10,#
          alphas= c(1/2^(c(6, 10, 30))),#
          burn=500,#
          init.method="Kmeans",#
          verbose=TRUE#
          ){#
#
      ## INNER FUNCTIONS#
      # Initial clustering#
      initiate_Z<-function(x, method=c("Kmeans", "random", "single") ){#
        if(method=="Kmeans"){#
        init.allocations<-  as.vector(kmeans(x, centers=k)$cluster)#
        } else if (method=="random"){#
          init.allocations<-  base::sample(c(1:k), n, replace=TRUE)#
        } else if (method=="single"){#
          init.allocations<-  rep(1, n)#
        }#
        return(init.allocations)#
      }#
      initiate_values<-function(ZZ){#
        IndiZ <-  (ZZ == matrix((1:k), nrow = n, ncol = k, byrow = T))#
        ns <-     apply(IndiZ,2,sum)  	#  size of each group#
        .Ysplit<- replicate(k, list())	#storage to group Y's#
        WkZ<-	    replicate(k, list(0))	#storage for within group variability#
        ybar<-	  replicate(k, list(0))#
        for (.i in 1:k){#
          .Ysplit[[.i]]<-y[ZZ==.i,]#
          if (ns[.i]>1){					# for groups with >1 obsevations#
            ybar[[.i]]<- as.matrix(t(apply(.Ysplit[[.i]], 2, mean) ))#
            } else if (ns[.i]==1){#
              ybar[[.i]]<-	t( as.matrix(.Ysplit[[.i]]))#
              } else {#
                ybar[[.i]]<-	NA#
              }#
          #Within group unexplained variability#
          if (ns[.i]==0) {#
            WkZ[[.i]]<-	NA#
            } else if (ns[.i]==1){#
              WkZ[[.i]]<-crossprod(as.matrix(.Ysplit[[.i]]-ybar[[.i]]))#
              } else {#
                for ( .n in 1:ns[.i]){#
                  WkZ[[.i]]<-WkZ[[.i]]+ crossprod( .Ysplit[[.i]][.n,]-ybar[[.i]])#
                }#
              }#
            }#
            list('ns'=ns,'ybar'=ybar, 'WkZ'=WkZ)#
        }#
      MuSample<-function(covs, ns_mu, WkZ_mu, ybar_mu){#
          # covs<-matrix(covs, nrow=r, byrow=T)#
          if (ns_mu==0) {#
            rmvn(1, b0, covs/n0)#
            } else {#
              bk<-(n0/(ns_mu+n0))*b0+(ns_mu/(n0+ns_mu))*ybar_mu#
              Bk<-(1/(ns_mu+n0))*covs#
              rmvn(1, t(bk), Bk)#
            }#
        }#
      Update_probs<-function(Mu, Cov, Pi ){#
          update_group_Prob <-function(x){#
              a1<-dmvn(y, Mu[[x]], Cov[[x]])#
              a2<-a1*Pi[x]#
              # for (i in 1:n)		{#
              #   if (sum(PZs[i,])==0) {#
              #     PZs[i,]<-rep(1/k,k)    # if all probs are zero, randomly allocate obs. very rare, might lead to crap results#
              #   }}#
              return(a2)#
            }#
          # apply to each component#
          ugp<-mapply(update_group_Prob, c(1:k))#
          # scale each row#
          ugp/apply(ugp, 1, sum)#
        }#
      CovSample<-function(ns_cov, WkZ_cov, ybar_cov){#
          c_cov<-c0+ns_cov#
          if (ns_cov==0) {#
            MCMCpack::riwish(c0, C0)#
          } else {#
            C_cov<- C0 +((ns_cov*n0)/(ns_cov+n0)*crossprod(ybar_cov-b0)) +WkZ_cov#
            MCMCpack::riwish(c_cov,C_cov)#
          }#
        }#
      Update_Zs<-function(PZs){mapply(function(x) sample( (1:k), 1, prob=PZs[x,]), c(1:n))}#
      parallelAccept<-function(w1, w2, a1, a2){#
          w1[w1< 1e-200]<-1e-200   # truncate so super small values dont crash everyting#
          w2[w2< 1e-200]<-1e-200#
            T1<-dDirichlet(w2, a1, log=TRUE)#
            T2<-dDirichlet(w1, a2, log=TRUE)#
            B1<-dDirichlet(w1, a1, log=TRUE)#
            B2<-dDirichlet(w2, a2, log=TRUE)#
            MH<-min(1,	exp(T1+T2-B1-B2))#
          Ax<-sample(c(1,0), 1, prob=c(MH,1-MH))#
          return(Ax)}#
      dDirichlet<-function (x, alpha, log = FALSE) {#
          dlog = lgamma(sum(alpha)) + sum((alpha - 1) * log(x)) - sum(lgamma(alpha))#
          result = ifelse(!log, exp(dlog), dlog)#
          return(result)#
      	}#
      # function#
 rdirichlet<-cmpfun(rdirichlet)#
#
## compute basic values#
  nCh<- length(alphas)#
  r<-   dim(y)[2]#
  n<-   dim(y)[1]#
  Loglike <-   rep(0,iterations)#
  SteadyScore<-data.frame("Iteration"=c(1:iterations), "K0"=0)#
#
# hyperpriors#
  n0<-  tau <-1   # this is tau equiv#
  Ck<-	replicate(k, list())#
  b0<-  apply(y,2,mean)#
  c0<-  r+1#
  C0<-  0.75*cov(y)#
  d<-   sum(c(1:r))+r#
#
## parameters to estimate#
## storage structure  par[[chain]][[iterations]][[k]]#
  Ps<-     replicate(nCh, replicate(iterations, list()))#
  Mus<-    replicate(nCh, replicate(iterations, list()))#
  Covs<-   replicate(nCh, replicate(iterations, list()))#
  Zs<-     replicate(nCh, replicate(iterations, list()))#
#
  if(verbose==TRUE){pb <- txtProgressBar(min = 0, max = iterations, style = 3)}#
#
# FOR EACH ITERATION#
for (.it in 1:iterations){  #for each iteration#
  # TRACKER#
if(verbose==TRUE && .it %% 10 == 0) {#
  Sys.sleep(0.01)#
  par(mfrow=c(2,1))#
  plot(SteadyScore$K0~SteadyScore$Iteration, main='#non-empty groups', type='l')#
  ts.plot( t(sapply(Ps[[nCh]], rbind)), main='Target Weights', col=rainbow(k))#
  Sys.sleep(0)#
  setTxtProgressBar(pb, .it)#
}#
#
  for (.ch in 1:nCh){   #for each chain#
#
# Input values#
    if (.it==1){#
      # iteration=1, initiallize input values#
      init.Z<-  initiate_Z(y, init.method)#
      input.values<-    initiate_values(init.Z) # compute values needed#
        ns<-  input.values$ns#
        ybar<-input.values$ybar#
        WkZ<- input.values$WkZ#
      } else {#
      # Update given current pars#
      input.values<-initiate_values(Zs[[.ch]][[.it-1]])  # check me#
        ns<-    input.values$ns                         # check me#
        ybar<-  input.values$ybar                       # check me#
        WkZ<-   input.values$WkZ                        # check me#
      }#
#
      # STEP 2.1 : GENERATE Samples for WEIGHTS from DIRICHLET dist#
      Ps[[.ch]][[.it]] <- rdirichlet(m=1, par= ns+alphas[.ch])#
#
      # STEP 2.2 GENERATE Samples from Covariance#
      Covs[[.ch]][[.it]]<-mapply(CovSample, ns, WkZ, ybar, SIMPLIFY=FALSE)#
#
      # STEP 2.3 GENERATE SAMPLEs of Means#
      Mus[[.ch]][[.it]]<- mapply(MuSample,Covs[[.ch]][[.it]], ns, WkZ, ybar, SIMPLIFY=FALSE)#
#
      # STEP 3.1: Draw new classification probabilities:#
      PZs<-Update_probs(Mus[[.ch]][[.it]], Covs[[.ch]][[.it]], Ps[[.ch]][[.it]])#
#
      # STEP 3.2: Update allocations based on probabilitie#
      Zs[[.ch]][[.it]]<-Update_Zs(PZs)#
#
} # end chain loop#
## PRIOR PARALLEL TEMPERING#
if(.it>20 && runif(1)<.9){#
  Chain1<-sample( 1:(nCh-1), 1) ; 	Chain2<-Chain1+1#
  MHratio<- parallelAccept(Ps[[Chain1]][[.it]], Ps[[Chain2]][[.it]], rep(alphas[Chain1],k), rep(alphas[Chain2],k))#
  if (MHratio==1){#
  # Flip the allocations#
  .z1<-	Zs[[Chain1]][[.it]] 	;.z2<-	Zs[[Chain2]][[.it]]#
  Zs[[Chain1]][[.it]]<-.z2 	;Zs[[Chain2]][[.it]]<-.z1#
#
  #Mu#
  .mu1<-	Mus[[Chain1]][[.it]] 	;.mu2<-	Mus[[Chain2]][[.it]]#
  Mus[[Chain1]][[.it]]<-.mu2 	;Mus[[Chain2]][[.it]]<-.mu1#
#
  #Cov#
  .cv1<-	Covs[[Chain1]][[.it]] 	;.cv2<-	Covs[[Chain2]][[.it]]#
  Covs[[Chain1]][[.it]]<-.cv2 	;Covs[[Chain2]][[.it]]<-.cv1#
#
  #Ps#
  .p1<-	Ps[[Chain1]][[.it]] 	;.p2<-	Ps[[Chain2]][[.it]]#
  Ps[[Chain1]][[.it]]<-.p2 	;Ps[[Chain2]][[.it]]<-.p1#
}#
}#
#
# count number of occupied components#
SteadyScore$K0[.it]<- factor(Zs[[.ch]][[.it]]) %>% levels(.) %>% length(.)#
#
# compute Log Likelihood ( still need to optomize)#
for (i in 1:n){#
  non0id<-Zs[[nCh]][[.it]] %>% factor() %>% levels() %>% as.numeric()#
    .ll<-0#
    for (numK in 1:length(non0id)){#
       .ll<-.ll+#
       Ps[[nCh]][[.it]][non0id[numK]]*dmvn(#
         y[i,], Mus[[nCh]][[.it]][[ non0id[numK] ]],#
         Covs[[nCh]][[.it]][[ non0id[numK] ]])#
    }#
  Loglike[.it]<-Loglike[.it]+ log(.ll)#
}#
} # end iteration loop#
if(verbose==TRUE){close(pb)}#
#
  burn<-burn+1#
	nCh                                     #number of chains#
	Mu.burned<-Mus[[nCh]][c(burn:iterations)]#
	Cov.burned<-Covs[[nCh]][burn:iterations]#
	Ps.burned<-Ps[[nCh]][burn:iterations]#
	Zs.burned<-Zs[[nCh]][burn:iterations]#
  Loglike.burned<-Loglike[burn:iterations]#
	SteadyScore.burned<-SteadyScore$K0[burn:iterations]#
#
# make weights and Zs matrices#
Zs.burned<-sapply(Zs.burned, rbind)#
Ps.burned<-sapply(Ps.burned, rbind)#
#
	return(list(#
    Mu = Mu.burned,#
    Cov=Cov.burned,#
    Ps= Ps.burned,#
    Zs=Zs.burned,#
    k.occupied=SteadyScore,#
    Log.likelihood=Loglike, y=y))#
}
test1<-Zmix_main(y,#
          iterations=100,#
          k=5,#
          alphas= c(1/2^(c(6, 10, 30))),#
          burn=20,#
          init.method="random"#
          )
test1
library("mvtnorm")#
library("dplyr")#
library("compiler")#
library("microbenchmark")#
library("ggplot2")#
library("mvnfast")#
#
Zmix_main<-function(#
          y,#
          iterations=2000,#
          k=10,#
          alphas= c(1/2^(c(6, 10, 30))),#
          burn=500,#
          init.method="Kmeans",#
          verbose=TRUE#
          ){#
#
      ## INNER FUNCTIONS#
      # Initial clustering#
      initiate_Z<-function(x, method=c("Kmeans", "random", "single") ){#
        if(method=="Kmeans"){#
        init.allocations<-  as.vector(kmeans(x, centers=k)$cluster)#
        } else if (method=="random"){#
          init.allocations<-  base::sample(c(1:k), n, replace=TRUE)#
        } else if (method=="single"){#
          init.allocations<-  rep(1, n)#
        }#
        return(init.allocations)#
      }#
      initiate_values<-function(ZZ){#
        IndiZ <-  (ZZ == matrix((1:k), nrow = n, ncol = k, byrow = T))#
        ns <-     apply(IndiZ,2,sum)  	#  size of each group#
        .Ysplit<- replicate(k, list())	#storage to group Y's#
        WkZ<-	    replicate(k, list(0))	#storage for within group variability#
        ybar<-	  replicate(k, list(0))#
        for (.i in 1:k){#
          .Ysplit[[.i]]<-y[ZZ==.i,]#
          if (ns[.i]>1){					# for groups with >1 obsevations#
            ybar[[.i]]<- as.matrix(t(apply(.Ysplit[[.i]], 2, mean) ))#
            } else if (ns[.i]==1){#
              ybar[[.i]]<-	t( as.matrix(.Ysplit[[.i]]))#
              } else {#
                ybar[[.i]]<-	NA#
              }#
          #Within group unexplained variability#
          if (ns[.i]==0) {#
            WkZ[[.i]]<-	NA#
            } else if (ns[.i]==1){#
              WkZ[[.i]]<-crossprod(as.matrix(.Ysplit[[.i]]-ybar[[.i]]))#
              } else {#
                for ( .n in 1:ns[.i]){#
                  WkZ[[.i]]<-WkZ[[.i]]+ crossprod( .Ysplit[[.i]][.n,]-ybar[[.i]])#
                }#
              }#
            }#
            list('ns'=ns,'ybar'=ybar, 'WkZ'=WkZ)#
        }#
      MuSample<-function(covs, ns_mu, WkZ_mu, ybar_mu){#
          # covs<-matrix(covs, nrow=r, byrow=T)#
          if (ns_mu==0) {#
            rmvn(1, b0, covs/n0)#
            } else {#
              bk<-(n0/(ns_mu+n0))*b0+(ns_mu/(n0+ns_mu))*ybar_mu#
              Bk<-(1/(ns_mu+n0))*covs#
              rmvn(1, t(bk), Bk)#
            }#
        }#
      Update_probs<-function(Mu, Cov, Pi ){#
          update_group_Prob <-function(x){#
              a1<-dmvn(y, Mu[[x]], Cov[[x]])#
              a2<-a1*Pi[x]#
              # for (i in 1:n)		{#
              #   if (sum(PZs[i,])==0) {#
              #     PZs[i,]<-rep(1/k,k)    # if all probs are zero, randomly allocate obs. very rare, might lead to crap results#
              #   }}#
              return(a2)#
            }#
          # apply to each component#
          ugp<-mapply(update_group_Prob, c(1:k))#
          # scale each row#
          ugp/apply(ugp, 1, sum)#
        }#
      CovSample<-function(ns_cov, WkZ_cov, ybar_cov){#
          c_cov<-c0+ns_cov#
          if (ns_cov==0) {#
            MCMCpack::riwish(c0, C0)#
          } else {#
            C_cov<- C0 +((ns_cov*n0)/(ns_cov+n0)*crossprod(ybar_cov-b0)) +WkZ_cov#
            MCMCpack::riwish(c_cov,C_cov)#
          }#
        }#
      Update_Zs<-function(PZs){mapply(function(x) sample( (1:k), 1, prob=PZs[x,]), c(1:n))}#
      parallelAccept<-function(w1, w2, a1, a2){#
          w1[w1< 1e-200]<-1e-200   # truncate so super small values dont crash everyting#
          w2[w2< 1e-200]<-1e-200#
            T1<-dDirichlet(w2, a1, log=TRUE)#
            T2<-dDirichlet(w1, a2, log=TRUE)#
            B1<-dDirichlet(w1, a1, log=TRUE)#
            B2<-dDirichlet(w2, a2, log=TRUE)#
            MH<-min(1,	exp(T1+T2-B1-B2))#
          Ax<-sample(c(1,0), 1, prob=c(MH,1-MH))#
          return(Ax)}#
      dDirichlet<-function (x, alpha, log = FALSE) {#
          dlog = lgamma(sum(alpha)) + sum((alpha - 1) * log(x)) - sum(lgamma(alpha))#
          result = ifelse(!log, exp(dlog), dlog)#
          return(result)#
      	}#
      # function#
 rdirichlet<-cmpfun(rdirichlet)#
#
## compute basic values#
  nCh<- length(alphas)#
  r<-   dim(y)[2]#
  n<-   dim(y)[1]#
  Loglike <-   rep(0,iterations)#
  SteadyScore<-data.frame("Iteration"=c(1:iterations), "K0"=0)#
#
# hyperpriors#
  n0<-  tau <-1   # this is tau equiv#
  Ck<-	replicate(k, list())#
  b0<-  apply(y,2,mean)#
  c0<-  r+1#
  C0<-  0.75*cov(y)#
  d<-   sum(c(1:r))+r#
#
## parameters to estimate#
## storage structure  par[[chain]][[iterations]][[k]]#
  Ps<-     replicate(nCh, replicate(iterations, list()))#
  Mus<-    replicate(nCh, replicate(iterations, list()))#
  Covs<-   replicate(nCh, replicate(iterations, list()))#
  Zs<-     replicate(nCh, replicate(iterations, list()))#
#
  if(verbose==TRUE){pb <- txtProgressBar(min = 0, max = iterations, style = 3)}#
#
# FOR EACH ITERATION#
for (.it in 1:iterations){  #for each iteration#
  # TRACKER#
if(verbose==TRUE && .it %% 10 == 0) {#
  Sys.sleep(0.01)#
  par(mfrow=c(2,1))#
  plot(SteadyScore$K0~SteadyScore$Iteration, main='#non-empty groups', type='l')#
  ts.plot( t(sapply(Ps[[nCh]], rbind)), main='Target Weights', col=rainbow(k))#
  Sys.sleep(0)#
  setTxtProgressBar(pb, .it)#
}#
#
  for (.ch in 1:nCh){   #for each chain#
#
# Input values#
    if (.it==1){#
      # iteration=1, initiallize input values#
      init.Z<-  initiate_Z(y, init.method)#
      input.values<-    initiate_values(init.Z) # compute values needed#
        ns<-  input.values$ns#
        ybar<-input.values$ybar#
        WkZ<- input.values$WkZ#
      } else {#
      # Update given current pars#
      input.values<-initiate_values(Zs[[.ch]][[.it-1]])  # check me#
        ns<-    input.values$ns                         # check me#
        ybar<-  input.values$ybar                       # check me#
        WkZ<-   input.values$WkZ                        # check me#
      }#
#
      # STEP 2.1 : GENERATE Samples for WEIGHTS from DIRICHLET dist#
      Ps[[.ch]][[.it]] <- rdirichlet(m=1, par= ns+alphas[.ch])#
#
      # STEP 2.2 GENERATE Samples from Covariance#
      Covs[[.ch]][[.it]]<-mapply(CovSample, ns, WkZ, ybar, SIMPLIFY=FALSE)#
#
      # STEP 2.3 GENERATE SAMPLEs of Means#
      Mus[[.ch]][[.it]]<- mapply(MuSample,Covs[[.ch]][[.it]], ns, WkZ, ybar, SIMPLIFY=FALSE)#
#
      # STEP 3.1: Draw new classification probabilities:#
      PZs<-Update_probs(Mus[[.ch]][[.it]], Covs[[.ch]][[.it]], Ps[[.ch]][[.it]])#
#
      # STEP 3.2: Update allocations based on probabilitie#
      Zs[[.ch]][[.it]]<-Update_Zs(PZs)#
#
} # end chain loop#
## PRIOR PARALLEL TEMPERING#
if(.it>20 && runif(1)<.9){#
  Chain1<-sample( 1:(nCh-1), 1) ; 	Chain2<-Chain1+1#
  MHratio<- parallelAccept(Ps[[Chain1]][[.it]], Ps[[Chain2]][[.it]], rep(alphas[Chain1],k), rep(alphas[Chain2],k))#
  if (MHratio==1){#
  # Flip the allocations#
  .z1<-	Zs[[Chain1]][[.it]] 	;.z2<-	Zs[[Chain2]][[.it]]#
  Zs[[Chain1]][[.it]]<-.z2 	;Zs[[Chain2]][[.it]]<-.z1#
#
  #Mu#
  .mu1<-	Mus[[Chain1]][[.it]] 	;.mu2<-	Mus[[Chain2]][[.it]]#
  Mus[[Chain1]][[.it]]<-.mu2 	;Mus[[Chain2]][[.it]]<-.mu1#
#
  #Cov#
  .cv1<-	Covs[[Chain1]][[.it]] 	;.cv2<-	Covs[[Chain2]][[.it]]#
  Covs[[Chain1]][[.it]]<-.cv2 	;Covs[[Chain2]][[.it]]<-.cv1#
#
  #Ps#
  .p1<-	Ps[[Chain1]][[.it]] 	;.p2<-	Ps[[Chain2]][[.it]]#
  Ps[[Chain1]][[.it]]<-.p2 	;Ps[[Chain2]][[.it]]<-.p1#
}#
}#
#
# count number of occupied components#
SteadyScore$K0[.it]<- factor(Zs[[.ch]][[.it]]) %>% levels(.) %>% length(.)#
#
# compute Log Likelihood ( still need to optomize)#
for (i in 1:n){#
  non0id<-Zs[[nCh]][[.it]] %>% factor() %>% levels() %>% as.numeric()#
    .ll<-0#
    for (numK in 1:length(non0id)){#
       .ll<-.ll+#
       Ps[[nCh]][[.it]][non0id[numK]]*dmvn(#
         y[i,], Mus[[nCh]][[.it]][[ non0id[numK] ]],#
         Covs[[nCh]][[.it]][[ non0id[numK] ]])#
    }#
  Loglike[.it]<-Loglike[.it]+ log(.ll)#
}#
} # end iteration loop#
if(verbose==TRUE){close(pb)}#
#
  burn<-burn+1#
	nCh                                     #number of chains#
	Mu.burned<-Mus[[nCh]][c(burn:iterations)]#
	Cov.burned<-Covs[[nCh]][burn:iterations]#
	Ps.burned<-Ps[[nCh]][burn:iterations]#
	Zs.burned<-Zs[[nCh]][burn:iterations]#
  Loglike.burned<-Loglike[burn:iterations]#
	SteadyScore.burned<-SteadyScore$K0[burn:iterations]#
#
# make weights and Zs matrices#
Zs.burned<-sapply(Zs.burned, rbind)#
Ps.burned<-sapply(Ps.burned, rbind)#
#
	return(list(#
    Mu = Mu.burned,#
    Cov=Cov.burned,#
    Ps= Ps.burned,#
    Zs=Zs.burned,#
    k.occupied=SteadyScore.burned,#
    Log.likelihood=Loglike, y=y))#
}
test1<-Zmix_main(y,#
          iterations=100,#
          k=5,#
          alphas= c(1/2^(c(6, 10, 30))),#
          burn=20,#
          init.method="random"#
          )
test1
library("mvtnorm")#
library("dplyr")#
library("compiler")#
library("microbenchmark")#
library("ggplot2")#
library("mvnfast")#
#
Zmix_main<-function(#
          y,#
          iterations=2000,#
          k=10,#
          alphas= c(1/2^(c(6, 10, 30))),#
          burn=500,#
          init.method="Kmeans",#
          verbose=TRUE#
          ){#
#
      ## INNER FUNCTIONS#
      # Initial clustering#
      initiate_Z<-function(x, method=c("Kmeans", "random", "single") ){#
        if(method=="Kmeans"){#
        init.allocations<-  as.vector(kmeans(x, centers=k)$cluster)#
        } else if (method=="random"){#
          init.allocations<-  base::sample(c(1:k), n, replace=TRUE)#
        } else if (method=="single"){#
          init.allocations<-  rep(1, n)#
        }#
        return(init.allocations)#
      }#
      initiate_values<-function(ZZ){#
        IndiZ <-  (ZZ == matrix((1:k), nrow = n, ncol = k, byrow = T))#
        ns <-     apply(IndiZ,2,sum)  	#  size of each group#
        .Ysplit<- replicate(k, list())	#storage to group Y's#
        WkZ<-	    replicate(k, list(0))	#storage for within group variability#
        ybar<-	  replicate(k, list(0))#
        for (.i in 1:k){#
          .Ysplit[[.i]]<-y[ZZ==.i,]#
          if (ns[.i]>1){					# for groups with >1 obsevations#
            ybar[[.i]]<- as.matrix(t(apply(.Ysplit[[.i]], 2, mean) ))#
            } else if (ns[.i]==1){#
              ybar[[.i]]<-	t( as.matrix(.Ysplit[[.i]]))#
              } else {#
                ybar[[.i]]<-	NA#
              }#
          #Within group unexplained variability#
          if (ns[.i]==0) {#
            WkZ[[.i]]<-	NA#
            } else if (ns[.i]==1){#
              WkZ[[.i]]<-crossprod(as.matrix(.Ysplit[[.i]]-ybar[[.i]]))#
              } else {#
                for ( .n in 1:ns[.i]){#
                  WkZ[[.i]]<-WkZ[[.i]]+ crossprod( .Ysplit[[.i]][.n,]-ybar[[.i]])#
                }#
              }#
            }#
            list('ns'=ns,'ybar'=ybar, 'WkZ'=WkZ)#
        }#
      MuSample<-function(covs, ns_mu, WkZ_mu, ybar_mu){#
          # covs<-matrix(covs, nrow=r, byrow=T)#
          if (ns_mu==0) {#
            rmvn(1, b0, covs/n0)#
            } else {#
              bk<-(n0/(ns_mu+n0))*b0+(ns_mu/(n0+ns_mu))*ybar_mu#
              Bk<-(1/(ns_mu+n0))*covs#
              rmvn(1, t(bk), Bk)#
            }#
        }#
      Update_probs<-function(Mu, Cov, Pi ){#
          update_group_Prob <-function(x){#
              a1<-dmvn(y, Mu[[x]], Cov[[x]])#
              a2<-a1*Pi[x]#
              # for (i in 1:n)		{#
              #   if (sum(PZs[i,])==0) {#
              #     PZs[i,]<-rep(1/k,k)    # if all probs are zero, randomly allocate obs. very rare, might lead to crap results#
              #   }}#
              return(a2)#
            }#
          # apply to each component#
          ugp<-mapply(update_group_Prob, c(1:k))#
          # scale each row#
          ugp/apply(ugp, 1, sum)#
        }#
      CovSample<-function(ns_cov, WkZ_cov, ybar_cov){#
          c_cov<-c0+ns_cov#
          if (ns_cov==0) {#
            MCMCpack::riwish(c0, C0)#
          } else {#
            C_cov<- C0 +((ns_cov*n0)/(ns_cov+n0)*crossprod(ybar_cov-b0)) +WkZ_cov#
            MCMCpack::riwish(c_cov,C_cov)#
          }#
        }#
      Update_Zs<-function(PZs){mapply(function(x) sample( (1:k), 1, prob=PZs[x,]), c(1:n))}#
      parallelAccept<-function(w1, w2, a1, a2){#
          w1[w1< 1e-200]<-1e-200   # truncate so super small values dont crash everyting#
          w2[w2< 1e-200]<-1e-200#
            T1<-dDirichlet(w2, a1, log=TRUE)#
            T2<-dDirichlet(w1, a2, log=TRUE)#
            B1<-dDirichlet(w1, a1, log=TRUE)#
            B2<-dDirichlet(w2, a2, log=TRUE)#
            MH<-min(1,	exp(T1+T2-B1-B2))#
          Ax<-sample(c(1,0), 1, prob=c(MH,1-MH))#
          return(Ax)}#
      dDirichlet<-function (x, alpha, log = FALSE) {#
          dlog = lgamma(sum(alpha)) + sum((alpha - 1) * log(x)) - sum(lgamma(alpha))#
          result = ifelse(!log, exp(dlog), dlog)#
          return(result)#
      	}#
      # function#
 rdirichlet<-cmpfun(rdirichlet)#
#
## compute basic values#
  nCh<- length(alphas)#
  r<-   dim(y)[2]#
  n<-   dim(y)[1]#
  Loglike <-   rep(0,iterations)#
  SteadyScore<-data.frame("Iteration"=c(1:iterations), "K0"=0)#
#
# hyperpriors#
  n0<-  tau <-1   # this is tau equiv#
  Ck<-	replicate(k, list())#
  b0<-  apply(y,2,mean)#
  c0<-  r+1#
  C0<-  0.75*cov(y)#
  d<-   sum(c(1:r))+r#
#
## parameters to estimate#
## storage structure  par[[chain]][[iterations]][[k]]#
  Ps<-     replicate(nCh, replicate(iterations, list()))#
  Mus<-    replicate(nCh, replicate(iterations, list()))#
  Covs<-   replicate(nCh, replicate(iterations, list()))#
  Zs<-     replicate(nCh, replicate(iterations, list()))#
#
  if(verbose==TRUE){pb <- txtProgressBar(min = 0, max = iterations, style = 3)}#
#
# FOR EACH ITERATION#
for (.it in 1:iterations){  #for each iteration#
  # TRACKER#
if(verbose==TRUE && .it %% 10 == 0) {#
  Sys.sleep(0.01)#
  par(mfrow=c(2,1))#
  plot(SteadyScore$K0~SteadyScore$Iteration, main='#non-empty groups', type='l')#
  ts.plot( t(sapply(Ps[[nCh]], rbind)), main='Target Weights', col=rainbow(k))#
  Sys.sleep(0)#
  setTxtProgressBar(pb, .it)#
}#
#
  for (.ch in 1:nCh){   #for each chain#
#
# Input values#
    if (.it==1){#
      # iteration=1, initiallize input values#
      init.Z<-  initiate_Z(y, init.method)#
      input.values<-    initiate_values(init.Z) # compute values needed#
        ns<-  input.values$ns#
        ybar<-input.values$ybar#
        WkZ<- input.values$WkZ#
      } else {#
      # Update given current pars#
      input.values<-initiate_values(Zs[[.ch]][[.it-1]])  # check me#
        ns<-    input.values$ns                         # check me#
        ybar<-  input.values$ybar                       # check me#
        WkZ<-   input.values$WkZ                        # check me#
      }#
#
      # STEP 2.1 : GENERATE Samples for WEIGHTS from DIRICHLET dist#
      Ps[[.ch]][[.it]] <- rdirichlet(m=1, par= ns+alphas[.ch])#
#
      # STEP 2.2 GENERATE Samples from Covariance#
      Covs[[.ch]][[.it]]<-mapply(CovSample, ns, WkZ, ybar, SIMPLIFY=FALSE)#
#
      # STEP 2.3 GENERATE SAMPLEs of Means#
      Mus[[.ch]][[.it]]<- mapply(MuSample,Covs[[.ch]][[.it]], ns, WkZ, ybar, SIMPLIFY=FALSE)#
#
      # STEP 3.1: Draw new classification probabilities:#
      PZs<-Update_probs(Mus[[.ch]][[.it]], Covs[[.ch]][[.it]], Ps[[.ch]][[.it]])#
#
      # STEP 3.2: Update allocations based on probabilitie#
      Zs[[.ch]][[.it]]<-Update_Zs(PZs)#
#
} # end chain loop#
## PRIOR PARALLEL TEMPERING#
if(.it>20 && runif(1)<.9){#
  Chain1<-sample( 1:(nCh-1), 1) ; 	Chain2<-Chain1+1#
  MHratio<- parallelAccept(Ps[[Chain1]][[.it]], Ps[[Chain2]][[.it]], rep(alphas[Chain1],k), rep(alphas[Chain2],k))#
  if (MHratio==1){#
  # Flip the allocations#
  .z1<-	Zs[[Chain1]][[.it]] 	;.z2<-	Zs[[Chain2]][[.it]]#
  Zs[[Chain1]][[.it]]<-.z2 	;Zs[[Chain2]][[.it]]<-.z1#
#
  #Mu#
  .mu1<-	Mus[[Chain1]][[.it]] 	;.mu2<-	Mus[[Chain2]][[.it]]#
  Mus[[Chain1]][[.it]]<-.mu2 	;Mus[[Chain2]][[.it]]<-.mu1#
#
  #Cov#
  .cv1<-	Covs[[Chain1]][[.it]] 	;.cv2<-	Covs[[Chain2]][[.it]]#
  Covs[[Chain1]][[.it]]<-.cv2 	;Covs[[Chain2]][[.it]]<-.cv1#
#
  #Ps#
  .p1<-	Ps[[Chain1]][[.it]] 	;.p2<-	Ps[[Chain2]][[.it]]#
  Ps[[Chain1]][[.it]]<-.p2 	;Ps[[Chain2]][[.it]]<-.p1#
}#
}#
#
# count number of occupied components#
SteadyScore$K0[.it]<- factor(Zs[[.ch]][[.it]]) %>% levels(.) %>% length(.)#
#
# compute Log Likelihood ( still need to optomize)#
for (i in 1:n){#
  non0id<-Zs[[nCh]][[.it]] %>% factor() %>% levels() %>% as.numeric()#
    .ll<-0#
    for (numK in 1:length(non0id)){#
       .ll<-.ll+#
       Ps[[nCh]][[.it]][non0id[numK]]*dmvn(#
         y[i,], Mus[[nCh]][[.it]][[ non0id[numK] ]],#
         Covs[[nCh]][[.it]][[ non0id[numK] ]])#
    }#
  Loglike[.it]<-Loglike[.it]+ log(.ll)#
}#
} # end iteration loop#
if(verbose==TRUE){close(pb)}#
#
  burn<-burn+1#
	nCh                                     #number of chains#
	Mu.burned<-Mus[[nCh]][c(burn:iterations)]#
	Cov.burned<-Covs[[nCh]][burn:iterations]#
	Ps.burned<-Ps[[nCh]][burn:iterations]#
	Zs.burned<-Zs[[nCh]][burn:iterations]#
  Loglike.burned<-Loglike[burn:iterations]#
	SteadyScore.burned<-SteadyScore$K0[burn:iterations]#
#
# make weights and Zs matrices#
Zs.burned<-sapply(Zs.burned, rbind)#
Ps.burned<-sapply(Ps.burned, cbind)#
#
	return(list(#
    Mu = Mu.burned,#
    Cov=Cov.burned,#
    Ps= Ps.burned,#
    Zs=Zs.burned,#
    k.occupied=SteadyScore.burned,#
    Log.likelihood=Loglike, y=y))#
}
test1<-Zmix_main(y,#
          iterations=100,#
          k=5,#
          alphas= c(1/2^(c(6, 10, 30))),#
          burn=20,#
          init.method="random"#
          )
test1[[1]]
test1[[2]]
test1[[3]]
test1[[4]]
library("mvtnorm")#
library("dplyr")#
library("compiler")#
library("microbenchmark")#
library("ggplot2")#
library("mvnfast")#
#
Zmix_main<-function(#
          y,#
          iterations=2000,#
          k=10,#
          alphas= c(1/2^(c(6, 10, 30))),#
          burn=500,#
          init.method="Kmeans",#
          verbose=TRUE#
          ){#
#
      ## INNER FUNCTIONS#
      # Initial clustering#
      initiate_Z<-function(x, method=c("Kmeans", "random", "single") ){#
        if(method=="Kmeans"){#
        init.allocations<-  as.vector(kmeans(x, centers=k)$cluster)#
        } else if (method=="random"){#
          init.allocations<-  base::sample(c(1:k), n, replace=TRUE)#
        } else if (method=="single"){#
          init.allocations<-  rep(1, n)#
        }#
        return(init.allocations)#
      }#
      initiate_values<-function(ZZ){#
        IndiZ <-  (ZZ == matrix((1:k), nrow = n, ncol = k, byrow = T))#
        ns <-     apply(IndiZ,2,sum)  	#  size of each group#
        .Ysplit<- replicate(k, list())	#storage to group Y's#
        WkZ<-	    replicate(k, list(0))	#storage for within group variability#
        ybar<-	  replicate(k, list(0))#
        for (.i in 1:k){#
          .Ysplit[[.i]]<-y[ZZ==.i,]#
          if (ns[.i]>1){					# for groups with >1 obsevations#
            ybar[[.i]]<- as.matrix(t(apply(.Ysplit[[.i]], 2, mean) ))#
            } else if (ns[.i]==1){#
              ybar[[.i]]<-	t( as.matrix(.Ysplit[[.i]]))#
              } else {#
                ybar[[.i]]<-	NA#
              }#
          #Within group unexplained variability#
          if (ns[.i]==0) {#
            WkZ[[.i]]<-	NA#
            } else if (ns[.i]==1){#
              WkZ[[.i]]<-crossprod(as.matrix(.Ysplit[[.i]]-ybar[[.i]]))#
              } else {#
                for ( .n in 1:ns[.i]){#
                  WkZ[[.i]]<-WkZ[[.i]]+ crossprod( .Ysplit[[.i]][.n,]-ybar[[.i]])#
                }#
              }#
            }#
            list('ns'=ns,'ybar'=ybar, 'WkZ'=WkZ)#
        }#
      MuSample<-function(covs, ns_mu, WkZ_mu, ybar_mu){#
          # covs<-matrix(covs, nrow=r, byrow=T)#
          if (ns_mu==0) {#
            rmvn(1, b0, covs/n0)#
            } else {#
              bk<-(n0/(ns_mu+n0))*b0+(ns_mu/(n0+ns_mu))*ybar_mu#
              Bk<-(1/(ns_mu+n0))*covs#
              rmvn(1, t(bk), Bk)#
            }#
        }#
      Update_probs<-function(Mu, Cov, Pi ){#
          update_group_Prob <-function(x){#
              a1<-dmvn(y, Mu[[x]], Cov[[x]])#
              a2<-a1*Pi[x]#
              # for (i in 1:n)		{#
              #   if (sum(PZs[i,])==0) {#
              #     PZs[i,]<-rep(1/k,k)    # if all probs are zero, randomly allocate obs. very rare, might lead to crap results#
              #   }}#
              return(a2)#
            }#
          # apply to each component#
          ugp<-mapply(update_group_Prob, c(1:k))#
          # scale each row#
          ugp/apply(ugp, 1, sum)#
        }#
      CovSample<-function(ns_cov, WkZ_cov, ybar_cov){#
          c_cov<-c0+ns_cov#
          if (ns_cov==0) {#
            MCMCpack::riwish(c0, C0)#
          } else {#
            C_cov<- C0 +((ns_cov*n0)/(ns_cov+n0)*crossprod(ybar_cov-b0)) +WkZ_cov#
            MCMCpack::riwish(c_cov,C_cov)#
          }#
        }#
      Update_Zs<-function(PZs){mapply(function(x) sample( (1:k), 1, prob=PZs[x,]), c(1:n))}#
      parallelAccept<-function(w1, w2, a1, a2){#
          w1[w1< 1e-200]<-1e-200   # truncate so super small values dont crash everyting#
          w2[w2< 1e-200]<-1e-200#
            T1<-dDirichlet(w2, a1, log=TRUE)#
            T2<-dDirichlet(w1, a2, log=TRUE)#
            B1<-dDirichlet(w1, a1, log=TRUE)#
            B2<-dDirichlet(w2, a2, log=TRUE)#
            MH<-min(1,	exp(T1+T2-B1-B2))#
          Ax<-sample(c(1,0), 1, prob=c(MH,1-MH))#
          return(Ax)}#
      dDirichlet<-function (x, alpha, log = FALSE) {#
          dlog = lgamma(sum(alpha)) + sum((alpha - 1) * log(x)) - sum(lgamma(alpha))#
          result = ifelse(!log, exp(dlog), dlog)#
          return(result)#
      	}#
      # function#
 rdirichlet<-cmpfun(rdirichlet)#
#
## compute basic values#
  nCh<- length(alphas)#
  r<-   dim(y)[2]#
  n<-   dim(y)[1]#
  Loglike <-   rep(0,iterations)#
  SteadyScore<-data.frame("Iteration"=c(1:iterations), "K0"=0)#
#
# hyperpriors#
  n0<-  tau <-1   # this is tau equiv#
  Ck<-	replicate(k, list())#
  b0<-  apply(y,2,mean)#
  c0<-  r+1#
  C0<-  0.75*cov(y)#
  d<-   sum(c(1:r))+r#
#
## parameters to estimate#
## storage structure  par[[chain]][[iterations]][[k]]#
  Ps<-     replicate(nCh, replicate(iterations, list()))#
  Mus<-    replicate(nCh, replicate(iterations, list()))#
  Covs<-   replicate(nCh, replicate(iterations, list()))#
  Zs<-     replicate(nCh, replicate(iterations, list()))#
#
  if(verbose==TRUE){pb <- txtProgressBar(min = 0, max = iterations, style = 3)}#
#
# FOR EACH ITERATION#
for (.it in 1:iterations){  #for each iteration#
  # TRACKER#
if(verbose==TRUE && .it %% 10 == 0) {#
  Sys.sleep(0.01)#
  par(mfrow=c(2,1))#
  plot(SteadyScore$K0~SteadyScore$Iteration, main='#non-empty groups', type='l')#
  ts.plot( t(sapply(Ps[[nCh]], rbind)), main='Target Weights', col=rainbow(k))#
  Sys.sleep(0)#
  setTxtProgressBar(pb, .it)#
}#
#
  for (.ch in 1:nCh){   #for each chain#
#
# Input values#
    if (.it==1){#
      # iteration=1, initiallize input values#
      init.Z<-  initiate_Z(y, init.method)#
      input.values<-    initiate_values(init.Z) # compute values needed#
        ns<-  input.values$ns#
        ybar<-input.values$ybar#
        WkZ<- input.values$WkZ#
      } else {#
      # Update given current pars#
      input.values<-initiate_values(Zs[[.ch]][[.it-1]])  # check me#
        ns<-    input.values$ns                         # check me#
        ybar<-  input.values$ybar                       # check me#
        WkZ<-   input.values$WkZ                        # check me#
      }#
#
      # STEP 2.1 : GENERATE Samples for WEIGHTS from DIRICHLET dist#
      Ps[[.ch]][[.it]] <- rdirichlet(m=1, par= ns+alphas[.ch])#
#
      # STEP 2.2 GENERATE Samples from Covariance#
      Covs[[.ch]][[.it]]<-mapply(CovSample, ns, WkZ, ybar, SIMPLIFY=FALSE)#
#
      # STEP 2.3 GENERATE SAMPLEs of Means#
      Mus[[.ch]][[.it]]<- mapply(MuSample,Covs[[.ch]][[.it]], ns, WkZ, ybar, SIMPLIFY=FALSE)#
#
      # STEP 3.1: Draw new classification probabilities:#
      PZs<-Update_probs(Mus[[.ch]][[.it]], Covs[[.ch]][[.it]], Ps[[.ch]][[.it]])#
#
      # STEP 3.2: Update allocations based on probabilitie#
      Zs[[.ch]][[.it]]<-Update_Zs(PZs)#
#
} # end chain loop#
## PRIOR PARALLEL TEMPERING#
if(.it>20 && runif(1)<.9){#
  Chain1<-sample( 1:(nCh-1), 1) ; 	Chain2<-Chain1+1#
  MHratio<- parallelAccept(Ps[[Chain1]][[.it]], Ps[[Chain2]][[.it]], rep(alphas[Chain1],k), rep(alphas[Chain2],k))#
  if (MHratio==1){#
  # Flip the allocations#
  .z1<-	Zs[[Chain1]][[.it]] 	;.z2<-	Zs[[Chain2]][[.it]]#
  Zs[[Chain1]][[.it]]<-.z2 	;Zs[[Chain2]][[.it]]<-.z1#
#
  #Mu#
  .mu1<-	Mus[[Chain1]][[.it]] 	;.mu2<-	Mus[[Chain2]][[.it]]#
  Mus[[Chain1]][[.it]]<-.mu2 	;Mus[[Chain2]][[.it]]<-.mu1#
#
  #Cov#
  .cv1<-	Covs[[Chain1]][[.it]] 	;.cv2<-	Covs[[Chain2]][[.it]]#
  Covs[[Chain1]][[.it]]<-.cv2 	;Covs[[Chain2]][[.it]]<-.cv1#
#
  #Ps#
  .p1<-	Ps[[Chain1]][[.it]] 	;.p2<-	Ps[[Chain2]][[.it]]#
  Ps[[Chain1]][[.it]]<-.p2 	;Ps[[Chain2]][[.it]]<-.p1#
}#
}#
#
# count number of occupied components#
SteadyScore$K0[.it]<- factor(Zs[[.ch]][[.it]]) %>% levels(.) %>% length(.)#
#
# compute Log Likelihood ( still need to optomize)#
for (i in 1:n){#
  non0id<-Zs[[nCh]][[.it]] %>% factor() %>% levels() %>% as.numeric()#
    .ll<-0#
    for (numK in 1:length(non0id)){#
       .ll<-.ll+#
       Ps[[nCh]][[.it]][non0id[numK]]*dmvn(#
         y[i,], Mus[[nCh]][[.it]][[ non0id[numK] ]],#
         Covs[[nCh]][[.it]][[ non0id[numK] ]])#
    }#
  Loglike[.it]<-Loglike[.it]+ log(.ll)#
}#
} # end iteration loop#
if(verbose==TRUE){close(pb)}#
#
  burn<-burn+1#
	nCh                                     #number of chains#
	Mu.burned<-Mus[[nCh]][c(burn:iterations)]#
	Cov.burned<-Covs[[nCh]][burn:iterations]#
	Ps.burned<-Ps[[nCh]][burn:iterations]#
	Zs.burned<-Zs[[nCh]][burn:iterations]#
  Loglike.burned<-Loglike[burn:iterations]#
	SteadyScore.burned<-SteadyScore$K0[burn:iterations]#
#
# make weights and Zs matrices#
Zs.burned<-t(sapply(Zs.burned, rbind))#
Ps.burned<-t(sapply(Ps.burned, cbind))#
#
	return(list(#
    Mu = Mu.burned,#
    Cov=Cov.burned,#
    Ps= Ps.burned,#
    Zs=Zs.burned,#
    k.occupied=SteadyScore.burned,#
    Log.likelihood=Loglike, y=y))#
}
test1<-Zmix_main(y,#
          iterations=100,#
          k=5,#
          alphas= c(1/2^(c(6, 10, 30))),#
          burn=20,#
          init.method="random"#
          )
test1[[3]]
ts.plot( test1[[3]])
ts.plot( test1[[4]])
image( test1[[4]])
test1[[1]]
y[1,]
y[,1]
test1<-Zmix_main(y[,1],#
          iterations=100,#
          k=5,#
          alphas= c(1/2^(c(6, 10, 30))),#
          burn=20,#
          init.method="random"#
          )
traceback()
library("mvtnorm")#
library("dplyr")#
library("compiler")#
library("microbenchmark")#
library("ggplot2")#
library("mvnfast")#
#
Zmix_main<-function(#
          y,#
          iterations=2000,#
          k=10,#
          alphas= c(1/2^(c(6, 10, 30))),#
          burn=500,#
          init.method="Kmeans",#
          verbose=TRUE#
          ){#
#
      ## INNER FUNCTIONS#
      # Initial clustering#
      initiate_Z<-function(x, method=c("Kmeans", "random", "single") ){#
        if(method=="Kmeans"){#
        init.allocations<-  as.vector(kmeans(x, centers=k)$cluster)#
        } else if (method=="random"){#
          init.allocations<-  base::sample(c(1:k), n, replace=TRUE)#
        } else if (method=="single"){#
          init.allocations<-  rep(1, n)#
        }#
        return(init.allocations)#
      }#
      initiate_values<-function(ZZ){#
        IndiZ <-  (ZZ == matrix((1:k), nrow = n, ncol = k, byrow = T))#
        ns <-     apply(IndiZ,2,sum)  	#  size of each group#
        .Ysplit<- replicate(k, list())	#storage to group Y's#
        WkZ<-	    replicate(k, list(0))	#storage for within group variability#
        ybar<-	  replicate(k, list(0))#
        for (.i in 1:k){#
          .Ysplit[[.i]]<-y[ZZ==.i,]#
          if (ns[.i]>1){					# for groups with >1 obsevations#
            ybar[[.i]]<- as.matrix(t(apply(.Ysplit[[.i]], 2, mean) ))  # UPDATE FOR UNIV#
            } else if (ns[.i]==1){#
              ybar[[.i]]<-	t( as.matrix(.Ysplit[[.i]]))#
              } else {#
                ybar[[.i]]<-	NA#
              }#
          #Within group unexplained variability#
          if (ns[.i]==0) {#
            WkZ[[.i]]<-	NA#
            } else if (ns[.i]==1){#
              WkZ[[.i]]<-crossprod(as.matrix(.Ysplit[[.i]]-ybar[[.i]]))#
              } else {#
                for ( .n in 1:ns[.i]){#
                  WkZ[[.i]]<-WkZ[[.i]]+ crossprod( .Ysplit[[.i]][.n,]-ybar[[.i]])#
                }#
              }#
            }#
            list('ns'=ns,'ybar'=ybar, 'WkZ'=WkZ)#
        }#
      MuSample<-function(covs, ns_mu, WkZ_mu, ybar_mu){#
          # covs<-matrix(covs, nrow=r, byrow=T)#
          if (ns_mu==0) {#
            rmvn(1, b0, covs/n0)#
            } else {#
              bk<-(n0/(ns_mu+n0))*b0+(ns_mu/(n0+ns_mu))*ybar_mu#
              Bk<-(1/(ns_mu+n0))*covs#
              rmvn(1, t(bk), Bk)#
            }#
        }#
      Update_probs<-function(Mu, Cov, Pi ){#
          update_group_Prob <-function(x){#
              a1<-dmvn(y, Mu[[x]], Cov[[x]])#
              a2<-a1*Pi[x]#
              # for (i in 1:n)		{#
              #   if (sum(PZs[i,])==0) {#
              #     PZs[i,]<-rep(1/k,k)    # if all probs are zero, randomly allocate obs. very rare, might lead to crap results#
              #   }}#
              return(a2)#
            }#
          # apply to each component#
          ugp<-mapply(update_group_Prob, c(1:k))#
          # scale each row#
          ugp/apply(ugp, 1, sum)#
        }#
      CovSample<-function(ns_cov, WkZ_cov, ybar_cov){#
          c_cov<-c0+ns_cov#
          if (ns_cov==0) {#
            MCMCpack::riwish(c0, C0)#
          } else {#
            C_cov<- C0 +((ns_cov*n0)/(ns_cov+n0)*crossprod(ybar_cov-b0)) +WkZ_cov#
            MCMCpack::riwish(c_cov,C_cov)#
          }#
        }#
      Update_Zs<-function(PZs){mapply(function(x) sample( (1:k), 1, prob=PZs[x,]), c(1:n))}#
      parallelAccept<-function(w1, w2, a1, a2){#
          w1[w1< 1e-200]<-1e-200   # truncate so super small values dont crash everyting#
          w2[w2< 1e-200]<-1e-200#
            T1<-dDirichlet(w2, a1, log=TRUE)#
            T2<-dDirichlet(w1, a2, log=TRUE)#
            B1<-dDirichlet(w1, a1, log=TRUE)#
            B2<-dDirichlet(w2, a2, log=TRUE)#
            MH<-min(1,	exp(T1+T2-B1-B2))#
          Ax<-sample(c(1,0), 1, prob=c(MH,1-MH))#
          return(Ax)}#
      dDirichlet<-function (x, alpha, log = FALSE) {#
          dlog = lgamma(sum(alpha)) + sum((alpha - 1) * log(x)) - sum(lgamma(alpha))#
          result = ifelse(!log, exp(dlog), dlog)#
          return(result)#
      	}#
      # function#
 rdirichlet<-cmpfun(rdirichlet)#
#
## compute basic values#
  nCh<- length(alphas)#
  r<-   dim(y)[2]#
  n<-   dim(y)[1]#
  Loglike <-   rep(0,iterations)#
  SteadyScore<-data.frame("Iteration"=c(1:iterations), "K0"=0)#
#
# hyperpriors#
  n0<-  tau <-1   # this is tau equiv#
  Ck<-	replicate(k, list())#
  c0<-  r+1#
  d<-   sum(c(1:r))+r#
if(r>1){#
  b0<-  apply(y,2,mean)#
  C0<-  0.75*cov(y)#
} else {#
  b0<-  mean(y)#
  C0<-  0.75*var(y)#
}#
## parameters to estimate#
## storage structure  par[[chain]][[iterations]][[k]]#
  Ps<-     replicate(nCh, replicate(iterations, list()))#
  Mus<-    replicate(nCh, replicate(iterations, list()))#
  Covs<-   replicate(nCh, replicate(iterations, list()))#
  Zs<-     replicate(nCh, replicate(iterations, list()))#
#
  if(verbose==TRUE){pb <- txtProgressBar(min = 0, max = iterations, style = 3)}#
#
# FOR EACH ITERATION#
for (.it in 1:iterations){  #for each iteration#
  # TRACKER#
if(verbose==TRUE && .it %% 10 == 0) {#
  Sys.sleep(0.01)#
  par(mfrow=c(2,1))#
  plot(SteadyScore$K0~SteadyScore$Iteration, main='#non-empty groups', type='l')#
  ts.plot( t(sapply(Ps[[nCh]], rbind)), main='Target Weights', col=rainbow(k))#
  Sys.sleep(0)#
  setTxtProgressBar(pb, .it)#
}#
#
  for (.ch in 1:nCh){   #for each chain#
#
# Input values#
    if (.it==1){#
      # iteration=1, initiallize input values#
      init.Z<-  initiate_Z(y, init.method)#
      input.values<-    initiate_values(init.Z) # compute values needed#
        ns<-  input.values$ns#
        ybar<-input.values$ybar#
        WkZ<- input.values$WkZ#
      } else {#
      # Update given current pars#
      input.values<-initiate_values(Zs[[.ch]][[.it-1]])  # check me#
        ns<-    input.values$ns                         # check me#
        ybar<-  input.values$ybar                       # check me#
        WkZ<-   input.values$WkZ                        # check me#
      }#
#
      # STEP 2.1 : GENERATE Samples for WEIGHTS from DIRICHLET dist#
      Ps[[.ch]][[.it]] <- rdirichlet(m=1, par= ns+alphas[.ch])#
#
      # STEP 2.2 GENERATE Samples from Covariance#
      Covs[[.ch]][[.it]]<-mapply(CovSample, ns, WkZ, ybar, SIMPLIFY=FALSE)#
#
      # STEP 2.3 GENERATE SAMPLEs of Means#
      Mus[[.ch]][[.it]]<- mapply(MuSample,Covs[[.ch]][[.it]], ns, WkZ, ybar, SIMPLIFY=FALSE)#
#
      # STEP 3.1: Draw new classification probabilities:#
      PZs<-Update_probs(Mus[[.ch]][[.it]], Covs[[.ch]][[.it]], Ps[[.ch]][[.it]])#
#
      # STEP 3.2: Update allocations based on probabilitie#
      Zs[[.ch]][[.it]]<-Update_Zs(PZs)#
#
} # end chain loop#
## PRIOR PARALLEL TEMPERING#
if(.it>20 && runif(1)<.9){#
  Chain1<-sample( 1:(nCh-1), 1) ; 	Chain2<-Chain1+1#
  MHratio<- parallelAccept(Ps[[Chain1]][[.it]], Ps[[Chain2]][[.it]], rep(alphas[Chain1],k), rep(alphas[Chain2],k))#
  if (MHratio==1){#
  # Flip the allocations#
  .z1<-	Zs[[Chain1]][[.it]] 	;.z2<-	Zs[[Chain2]][[.it]]#
  Zs[[Chain1]][[.it]]<-.z2 	;Zs[[Chain2]][[.it]]<-.z1#
#
  #Mu#
  .mu1<-	Mus[[Chain1]][[.it]] 	;.mu2<-	Mus[[Chain2]][[.it]]#
  Mus[[Chain1]][[.it]]<-.mu2 	;Mus[[Chain2]][[.it]]<-.mu1#
#
  #Cov#
  .cv1<-	Covs[[Chain1]][[.it]] 	;.cv2<-	Covs[[Chain2]][[.it]]#
  Covs[[Chain1]][[.it]]<-.cv2 	;Covs[[Chain2]][[.it]]<-.cv1#
#
  #Ps#
  .p1<-	Ps[[Chain1]][[.it]] 	;.p2<-	Ps[[Chain2]][[.it]]#
  Ps[[Chain1]][[.it]]<-.p2 	;Ps[[Chain2]][[.it]]<-.p1#
}#
}#
#
# count number of occupied components#
SteadyScore$K0[.it]<- factor(Zs[[.ch]][[.it]]) %>% levels(.) %>% length(.)#
#
# compute Log Likelihood ( still need to optomize)#
for (i in 1:n){#
  non0id<-Zs[[nCh]][[.it]] %>% factor() %>% levels() %>% as.numeric()#
    .ll<-0#
    for (numK in 1:length(non0id)){#
       .ll<-.ll+#
       Ps[[nCh]][[.it]][non0id[numK]]*dmvn(#
         y[i,], Mus[[nCh]][[.it]][[ non0id[numK] ]],#
         Covs[[nCh]][[.it]][[ non0id[numK] ]])#
    }#
  Loglike[.it]<-Loglike[.it]+ log(.ll)#
}#
} # end iteration loop#
if(verbose==TRUE){close(pb)}#
#
  burn<-burn+1#
	nCh                                     #number of chains#
	Mu.burned<-Mus[[nCh]][burn:iterations]#
	Cov.burned<-Covs[[nCh]][burn:iterations]#
	Ps.burned<-Ps[[nCh]][burn:iterations]#
	Zs.burned<-Zs[[nCh]][burn:iterations]#
  Loglike.burned<-Loglike[burn:iterations]#
	SteadyScore.burned<-SteadyScore$K0[burn:iterations]#
#
# make weights and Zs matrices#
Zs.burned<-t(sapply(Zs.burned, rbind))#
Ps.burned<-t(sapply(Ps.burned, cbind))#
#
	return(list(#
    Mu = Mu.burned,#
    Cov=Cov.burned,#
    Ps= Ps.burned,#
    Zs=Zs.burned,#
    k.occupied=SteadyScore.burned,#
    Log.likelihood=Loglike, y=y))#
}
test1<-Zmix_main(y[,1],#
          iterations=100,#
          k=5,#
          alphas= c(1/2^(c(6, 10, 30))),#
          burn=20,#
          init.method="random"#
          )
library("mvtnorm")#
library("dplyr")#
library("compiler")#
library("microbenchmark")#
library("ggplot2")#
library("mvnfast")#
#
Zmix_main<-function(#
          y,#
          iterations=2000,#
          k=10,#
          alphas= c(1/2^(c(6, 10, 30))),#
          burn=500,#
          init.method="Kmeans",#
          verbose=TRUE#
          ){#
#
      ## INNER FUNCTIONS#
      # Initial clustering#
      initiate_Z<-function(x, method=c("Kmeans", "random", "single") ){#
        if(method=="Kmeans"){#
        init.allocations<-  as.vector(kmeans(x, centers=k)$cluster)#
        } else if (method=="random"){#
          init.allocations<-  base::sample(c(1:k), n, replace=TRUE)#
        } else if (method=="single"){#
          init.allocations<-  rep(1, n)#
        }#
        return(init.allocations)#
      }#
      initiate_values<-function(ZZ){#
        IndiZ <-  (ZZ == matrix((1:k), nrow = n, ncol = k, byrow = T))#
        ns <-     apply(IndiZ,2,sum)  	#  size of each group#
        .Ysplit<- replicate(k, list())	#storage to group Y's#
        WkZ<-	    replicate(k, list(0))	#storage for within group variability#
        ybar<-	  replicate(k, list(0))#
        for (.i in 1:k){#
          .Ysplit[[.i]]<-y[ZZ==.i,]#
          if (ns[.i]>1){					# for groups with >1 obsevations#
            ybar[[.i]]<- as.matrix(t(apply(.Ysplit[[.i]], 2, mean) ))  # UPDATE FOR UNIV#
            } else if (ns[.i]==1){#
              ybar[[.i]]<-	t( as.matrix(.Ysplit[[.i]]))#
              } else {#
                ybar[[.i]]<-	NA#
              }#
          #Within group unexplained variability#
          if (ns[.i]==0) {#
            WkZ[[.i]]<-	NA#
            } else if (ns[.i]==1){#
              WkZ[[.i]]<-crossprod(as.matrix(.Ysplit[[.i]]-ybar[[.i]]))#
              } else {#
                for ( .n in 1:ns[.i]){#
                  WkZ[[.i]]<-WkZ[[.i]]+ crossprod( .Ysplit[[.i]][.n,]-ybar[[.i]])#
                }#
              }#
            }#
            list('ns'=ns,'ybar'=ybar, 'WkZ'=WkZ)#
        }#
      MuSample<-function(covs, ns_mu, WkZ_mu, ybar_mu){#
          # covs<-matrix(covs, nrow=r, byrow=T)#
          if (ns_mu==0) {#
            rmvn(1, b0, covs/n0)#
            } else {#
              bk<-(n0/(ns_mu+n0))*b0+(ns_mu/(n0+ns_mu))*ybar_mu#
              Bk<-(1/(ns_mu+n0))*covs#
              rmvn(1, t(bk), Bk)#
            }#
        }#
      Update_probs<-function(Mu, Cov, Pi ){#
          update_group_Prob <-function(x){#
              a1<-dmvn(y, Mu[[x]], Cov[[x]])#
              a2<-a1*Pi[x]#
              # for (i in 1:n)		{#
              #   if (sum(PZs[i,])==0) {#
              #     PZs[i,]<-rep(1/k,k)    # if all probs are zero, randomly allocate obs. very rare, might lead to crap results#
              #   }}#
              return(a2)#
            }#
          # apply to each component#
          ugp<-mapply(update_group_Prob, c(1:k))#
          # scale each row#
          ugp/apply(ugp, 1, sum)#
        }#
      CovSample<-function(ns_cov, WkZ_cov, ybar_cov){#
          c_cov<-c0+ns_cov#
          if (ns_cov==0) {#
            MCMCpack::riwish(c0, C0)#
          } else {#
            C_cov<- C0 +((ns_cov*n0)/(ns_cov+n0)*crossprod(ybar_cov-b0)) +WkZ_cov#
            MCMCpack::riwish(c_cov,C_cov)#
          }#
        }#
      Update_Zs<-function(PZs){mapply(function(x) sample( (1:k), 1, prob=PZs[x,]), c(1:n))}#
      parallelAccept<-function(w1, w2, a1, a2){#
          w1[w1< 1e-200]<-1e-200   # truncate so super small values dont crash everyting#
          w2[w2< 1e-200]<-1e-200#
            T1<-dDirichlet(w2, a1, log=TRUE)#
            T2<-dDirichlet(w1, a2, log=TRUE)#
            B1<-dDirichlet(w1, a1, log=TRUE)#
            B2<-dDirichlet(w2, a2, log=TRUE)#
            MH<-min(1,	exp(T1+T2-B1-B2))#
          Ax<-sample(c(1,0), 1, prob=c(MH,1-MH))#
          return(Ax)}#
      dDirichlet<-function (x, alpha, log = FALSE) {#
          dlog = lgamma(sum(alpha)) + sum((alpha - 1) * log(x)) - sum(lgamma(alpha))#
          result = ifelse(!log, exp(dlog), dlog)#
          return(result)#
      	}#
      # function#
 rdirichlet<-cmpfun(rdirichlet)#
#
## compute basic values#
  nCh<- length(alphas)#
  r<-   dim(y)[2]#
  n<-   dim(y)[1]#
  Loglike <-   rep(0,iterations)#
  SteadyScore<-data.frame("Iteration"=c(1:iterations), "K0"=0)#
#
# hyperpriors#
  n0<-  tau <-1   # this is tau equiv#
  Ck<-	replicate(k, list())#
  c0<-  r+1#
if(r>1){#
  b0<-  apply(y,2,mean)#
  C0<-  0.75*cov(y)#
  d<-   sum(c(1:r))+r#
} else {#
  b0<-  mean(y)#
  C0<-  0.75*var(y)#
  d<-2#
}#
## parameters to estimate#
## storage structure  par[[chain]][[iterations]][[k]]#
  Ps<-     replicate(nCh, replicate(iterations, list()))#
  Mus<-    replicate(nCh, replicate(iterations, list()))#
  Covs<-   replicate(nCh, replicate(iterations, list()))#
  Zs<-     replicate(nCh, replicate(iterations, list()))#
#
  if(verbose==TRUE){pb <- txtProgressBar(min = 0, max = iterations, style = 3)}#
#
# FOR EACH ITERATION#
for (.it in 1:iterations){  #for each iteration#
  # TRACKER#
if(verbose==TRUE && .it %% 10 == 0) {#
  Sys.sleep(0.01)#
  par(mfrow=c(2,1))#
  plot(SteadyScore$K0~SteadyScore$Iteration, main='#non-empty groups', type='l')#
  ts.plot( t(sapply(Ps[[nCh]], rbind)), main='Target Weights', col=rainbow(k))#
  Sys.sleep(0)#
  setTxtProgressBar(pb, .it)#
}#
#
  for (.ch in 1:nCh){   #for each chain#
#
# Input values#
    if (.it==1){#
      # iteration=1, initiallize input values#
      init.Z<-  initiate_Z(y, init.method)#
      input.values<-    initiate_values(init.Z) # compute values needed#
        ns<-  input.values$ns#
        ybar<-input.values$ybar#
        WkZ<- input.values$WkZ#
      } else {#
      # Update given current pars#
      input.values<-initiate_values(Zs[[.ch]][[.it-1]])  # check me#
        ns<-    input.values$ns                         # check me#
        ybar<-  input.values$ybar                       # check me#
        WkZ<-   input.values$WkZ                        # check me#
      }#
#
      # STEP 2.1 : GENERATE Samples for WEIGHTS from DIRICHLET dist#
      Ps[[.ch]][[.it]] <- rdirichlet(m=1, par= ns+alphas[.ch])#
#
      # STEP 2.2 GENERATE Samples from Covariance#
      Covs[[.ch]][[.it]]<-mapply(CovSample, ns, WkZ, ybar, SIMPLIFY=FALSE)#
#
      # STEP 2.3 GENERATE SAMPLEs of Means#
      Mus[[.ch]][[.it]]<- mapply(MuSample,Covs[[.ch]][[.it]], ns, WkZ, ybar, SIMPLIFY=FALSE)#
#
      # STEP 3.1: Draw new classification probabilities:#
      PZs<-Update_probs(Mus[[.ch]][[.it]], Covs[[.ch]][[.it]], Ps[[.ch]][[.it]])#
#
      # STEP 3.2: Update allocations based on probabilitie#
      Zs[[.ch]][[.it]]<-Update_Zs(PZs)#
#
} # end chain loop#
## PRIOR PARALLEL TEMPERING#
if(.it>20 && runif(1)<.9){#
  Chain1<-sample( 1:(nCh-1), 1) ; 	Chain2<-Chain1+1#
  MHratio<- parallelAccept(Ps[[Chain1]][[.it]], Ps[[Chain2]][[.it]], rep(alphas[Chain1],k), rep(alphas[Chain2],k))#
  if (MHratio==1){#
  # Flip the allocations#
  .z1<-	Zs[[Chain1]][[.it]] 	;.z2<-	Zs[[Chain2]][[.it]]#
  Zs[[Chain1]][[.it]]<-.z2 	;Zs[[Chain2]][[.it]]<-.z1#
#
  #Mu#
  .mu1<-	Mus[[Chain1]][[.it]] 	;.mu2<-	Mus[[Chain2]][[.it]]#
  Mus[[Chain1]][[.it]]<-.mu2 	;Mus[[Chain2]][[.it]]<-.mu1#
#
  #Cov#
  .cv1<-	Covs[[Chain1]][[.it]] 	;.cv2<-	Covs[[Chain2]][[.it]]#
  Covs[[Chain1]][[.it]]<-.cv2 	;Covs[[Chain2]][[.it]]<-.cv1#
#
  #Ps#
  .p1<-	Ps[[Chain1]][[.it]] 	;.p2<-	Ps[[Chain2]][[.it]]#
  Ps[[Chain1]][[.it]]<-.p2 	;Ps[[Chain2]][[.it]]<-.p1#
}#
}#
#
# count number of occupied components#
SteadyScore$K0[.it]<- factor(Zs[[.ch]][[.it]]) %>% levels(.) %>% length(.)#
#
# compute Log Likelihood ( still need to optomize)#
for (i in 1:n){#
  non0id<-Zs[[nCh]][[.it]] %>% factor() %>% levels() %>% as.numeric()#
    .ll<-0#
    for (numK in 1:length(non0id)){#
       .ll<-.ll+#
       Ps[[nCh]][[.it]][non0id[numK]]*dmvn(#
         y[i,], Mus[[nCh]][[.it]][[ non0id[numK] ]],#
         Covs[[nCh]][[.it]][[ non0id[numK] ]])#
    }#
  Loglike[.it]<-Loglike[.it]+ log(.ll)#
}#
} # end iteration loop#
if(verbose==TRUE){close(pb)}#
#
  burn<-burn+1#
	nCh                                     #number of chains#
	Mu.burned<-Mus[[nCh]][burn:iterations]#
	Cov.burned<-Covs[[nCh]][burn:iterations]#
	Ps.burned<-Ps[[nCh]][burn:iterations]#
	Zs.burned<-Zs[[nCh]][burn:iterations]#
  Loglike.burned<-Loglike[burn:iterations]#
	SteadyScore.burned<-SteadyScore$K0[burn:iterations]#
#
# make weights and Zs matrices#
Zs.burned<-t(sapply(Zs.burned, rbind))#
Ps.burned<-t(sapply(Ps.burned, cbind))#
#
	return(list(#
    Mu = Mu.burned,#
    Cov=Cov.burned,#
    Ps= Ps.burned,#
    Zs=Zs.burned,#
    k.occupied=SteadyScore.burned,#
    Log.likelihood=Loglike, y=y))#
}
test1<-Zmix_main(y[,1],#
          iterations=100,#
          k=5,#
          alphas= c(1/2^(c(6, 10, 30))),#
          burn=20,#
          init.method="random"#
          )
library("mvtnorm")#
library("dplyr")#
library("compiler")#
library("microbenchmark")#
library("ggplot2")#
library("mvnfast")#
#
Zmix_main<-function(#
          y,#
          iterations=2000,#
          k=10,#
          alphas= c(1/2^(c(6, 10, 30))),#
          burn=500,#
          init.method="Kmeans",#
          verbose=TRUE#
          ){#
#
      ## INNER FUNCTIONS#
      # Initial clustering#
      initiate_Z<-function(x, method=c("Kmeans", "random", "single") ){#
        if(method=="Kmeans"){#
        init.allocations<-  as.vector(kmeans(x, centers=k)$cluster)#
        } else if (method=="random"){#
          init.allocations<-  base::sample(c(1:k), n, replace=TRUE)#
        } else if (method=="single"){#
          init.allocations<-  rep(1, n)#
        }#
        return(init.allocations)#
      }#
      initiate_values<-function(ZZ){#
        IndiZ <-  (ZZ == matrix((1:k), nrow = n, ncol = k, byrow = T))#
        ns <-     apply(IndiZ,2,sum)  	#  size of each group#
        .Ysplit<- replicate(k, list())	#storage to group Y's#
        WkZ<-	    replicate(k, list(0))	#storage for within group variability#
        ybar<-	  replicate(k, list(0))#
        for (.i in 1:k){#
          .Ysplit[[.i]]<-y[ZZ==.i,]#
          if (ns[.i]>1){					# for groups with >1 obsevations#
            ybar[[.i]]<- as.matrix(t(apply(.Ysplit[[.i]], 2, mean) ))  # UPDATE FOR UNIV#
            } else if (ns[.i]==1){#
              ybar[[.i]]<-	t( as.matrix(.Ysplit[[.i]]))#
              } else {#
                ybar[[.i]]<-	NA#
              }#
          #Within group unexplained variability#
          if (ns[.i]==0) {#
            WkZ[[.i]]<-	NA#
            } else if (ns[.i]==1){#
              WkZ[[.i]]<-crossprod(as.matrix(.Ysplit[[.i]]-ybar[[.i]]))#
              } else {#
                for ( .n in 1:ns[.i]){#
                  WkZ[[.i]]<-WkZ[[.i]]+ crossprod( .Ysplit[[.i]][.n,]-ybar[[.i]])#
                }#
              }#
            }#
            list('ns'=ns,'ybar'=ybar, 'WkZ'=WkZ)#
        }#
      MuSample<-function(covs, ns_mu, WkZ_mu, ybar_mu){#
          # covs<-matrix(covs, nrow=r, byrow=T)#
          if (ns_mu==0) {#
            rmvn(1, b0, covs/n0)#
            } else {#
              bk<-(n0/(ns_mu+n0))*b0+(ns_mu/(n0+ns_mu))*ybar_mu#
              Bk<-(1/(ns_mu+n0))*covs#
              rmvn(1, t(bk), Bk)#
            }#
        }#
      Update_probs<-function(Mu, Cov, Pi ){#
          update_group_Prob <-function(x){#
              a1<-dmvn(y, Mu[[x]], Cov[[x]])#
              a2<-a1*Pi[x]#
              # for (i in 1:n)		{#
              #   if (sum(PZs[i,])==0) {#
              #     PZs[i,]<-rep(1/k,k)    # if all probs are zero, randomly allocate obs. very rare, might lead to crap results#
              #   }}#
              return(a2)#
            }#
          # apply to each component#
          ugp<-mapply(update_group_Prob, c(1:k))#
          # scale each row#
          ugp/apply(ugp, 1, sum)#
        }#
      CovSample<-function(ns_cov, WkZ_cov, ybar_cov){#
          c_cov<-c0+ns_cov#
          if (ns_cov==0) {#
            MCMCpack::riwish(c0, C0)#
          } else {#
            C_cov<- C0 +((ns_cov*n0)/(ns_cov+n0)*crossprod(ybar_cov-b0)) +WkZ_cov#
            MCMCpack::riwish(c_cov,C_cov)#
          }#
        }#
      Update_Zs<-function(PZs){mapply(function(x) sample( (1:k), 1, prob=PZs[x,]), c(1:n))}#
      parallelAccept<-function(w1, w2, a1, a2){#
          w1[w1< 1e-200]<-1e-200   # truncate so super small values dont crash everyting#
          w2[w2< 1e-200]<-1e-200#
            T1<-dDirichlet(w2, a1, log=TRUE)#
            T2<-dDirichlet(w1, a2, log=TRUE)#
            B1<-dDirichlet(w1, a1, log=TRUE)#
            B2<-dDirichlet(w2, a2, log=TRUE)#
            MH<-min(1,	exp(T1+T2-B1-B2))#
          Ax<-sample(c(1,0), 1, prob=c(MH,1-MH))#
          return(Ax)}#
      dDirichlet<-function (x, alpha, log = FALSE) {#
          dlog = lgamma(sum(alpha)) + sum((alpha - 1) * log(x)) - sum(lgamma(alpha))#
          result = ifelse(!log, exp(dlog), dlog)#
          return(result)#
      	}#
      # function#
 rdirichlet<-cmpfun(rdirichlet)#
#
## compute basic values#
  nCh<- length(alphas)#
  if(is.vector(y)){#
    r<-1#
    n<-length(y)#
  } else {#
    r<-   dim(y)[2]#
    n<-   dim(y)[1]#
  }#
  Loglike <-   rep(0,iterations)#
  SteadyScore<-data.frame("Iteration"=c(1:iterations), "K0"=0)#
#
# hyperpriors#
  n0<-  tau <-1   # this is tau equiv#
  Ck<-	replicate(k, list())#
  c0<-  r+1#
if(r>1){#
  b0<-  apply(y,2,mean)#
  C0<-  0.75*cov(y)#
  d<-   sum(c(1:r))+r#
} else {#
  b0<-  mean(y)#
  C0<-  0.75*var(y)#
  d<-2#
}#
## parameters to estimate#
## storage structure  par[[chain]][[iterations]][[k]]#
  Ps<-     replicate(nCh, replicate(iterations, list()))#
  Mus<-    replicate(nCh, replicate(iterations, list()))#
  Covs<-   replicate(nCh, replicate(iterations, list()))#
  Zs<-     replicate(nCh, replicate(iterations, list()))#
#
  if(verbose==TRUE){pb <- txtProgressBar(min = 0, max = iterations, style = 3)}#
#
# FOR EACH ITERATION#
for (.it in 1:iterations){  #for each iteration#
  # TRACKER#
if(verbose==TRUE && .it %% 10 == 0) {#
  Sys.sleep(0.01)#
  par(mfrow=c(2,1))#
  plot(SteadyScore$K0~SteadyScore$Iteration, main='#non-empty groups', type='l')#
  ts.plot( t(sapply(Ps[[nCh]], rbind)), main='Target Weights', col=rainbow(k))#
  Sys.sleep(0)#
  setTxtProgressBar(pb, .it)#
}#
#
  for (.ch in 1:nCh){   #for each chain#
#
# Input values#
    if (.it==1){#
      # iteration=1, initiallize input values#
      init.Z<-  initiate_Z(y, init.method)#
      input.values<-    initiate_values(init.Z) # compute values needed#
        ns<-  input.values$ns#
        ybar<-input.values$ybar#
        WkZ<- input.values$WkZ#
      } else {#
      # Update given current pars#
      input.values<-initiate_values(Zs[[.ch]][[.it-1]])  # check me#
        ns<-    input.values$ns                         # check me#
        ybar<-  input.values$ybar                       # check me#
        WkZ<-   input.values$WkZ                        # check me#
      }#
#
      # STEP 2.1 : GENERATE Samples for WEIGHTS from DIRICHLET dist#
      Ps[[.ch]][[.it]] <- rdirichlet(m=1, par= ns+alphas[.ch])#
#
      # STEP 2.2 GENERATE Samples from Covariance#
      Covs[[.ch]][[.it]]<-mapply(CovSample, ns, WkZ, ybar, SIMPLIFY=FALSE)#
#
      # STEP 2.3 GENERATE SAMPLEs of Means#
      Mus[[.ch]][[.it]]<- mapply(MuSample,Covs[[.ch]][[.it]], ns, WkZ, ybar, SIMPLIFY=FALSE)#
#
      # STEP 3.1: Draw new classification probabilities:#
      PZs<-Update_probs(Mus[[.ch]][[.it]], Covs[[.ch]][[.it]], Ps[[.ch]][[.it]])#
#
      # STEP 3.2: Update allocations based on probabilitie#
      Zs[[.ch]][[.it]]<-Update_Zs(PZs)#
#
} # end chain loop#
## PRIOR PARALLEL TEMPERING#
if(.it>20 && runif(1)<.9){#
  Chain1<-sample( 1:(nCh-1), 1) ; 	Chain2<-Chain1+1#
  MHratio<- parallelAccept(Ps[[Chain1]][[.it]], Ps[[Chain2]][[.it]], rep(alphas[Chain1],k), rep(alphas[Chain2],k))#
  if (MHratio==1){#
  # Flip the allocations#
  .z1<-	Zs[[Chain1]][[.it]] 	;.z2<-	Zs[[Chain2]][[.it]]#
  Zs[[Chain1]][[.it]]<-.z2 	;Zs[[Chain2]][[.it]]<-.z1#
#
  #Mu#
  .mu1<-	Mus[[Chain1]][[.it]] 	;.mu2<-	Mus[[Chain2]][[.it]]#
  Mus[[Chain1]][[.it]]<-.mu2 	;Mus[[Chain2]][[.it]]<-.mu1#
#
  #Cov#
  .cv1<-	Covs[[Chain1]][[.it]] 	;.cv2<-	Covs[[Chain2]][[.it]]#
  Covs[[Chain1]][[.it]]<-.cv2 	;Covs[[Chain2]][[.it]]<-.cv1#
#
  #Ps#
  .p1<-	Ps[[Chain1]][[.it]] 	;.p2<-	Ps[[Chain2]][[.it]]#
  Ps[[Chain1]][[.it]]<-.p2 	;Ps[[Chain2]][[.it]]<-.p1#
}#
}#
#
# count number of occupied components#
SteadyScore$K0[.it]<- factor(Zs[[.ch]][[.it]]) %>% levels(.) %>% length(.)#
#
# compute Log Likelihood ( still need to optomize)#
for (i in 1:n){#
  non0id<-Zs[[nCh]][[.it]] %>% factor() %>% levels() %>% as.numeric()#
    .ll<-0#
    for (numK in 1:length(non0id)){#
       .ll<-.ll+#
       Ps[[nCh]][[.it]][non0id[numK]]*dmvn(#
         y[i,], Mus[[nCh]][[.it]][[ non0id[numK] ]],#
         Covs[[nCh]][[.it]][[ non0id[numK] ]])#
    }#
  Loglike[.it]<-Loglike[.it]+ log(.ll)#
}#
} # end iteration loop#
if(verbose==TRUE){close(pb)}#
#
  burn<-burn+1#
	nCh                                     #number of chains#
	Mu.burned<-Mus[[nCh]][burn:iterations]#
	Cov.burned<-Covs[[nCh]][burn:iterations]#
	Ps.burned<-Ps[[nCh]][burn:iterations]#
	Zs.burned<-Zs[[nCh]][burn:iterations]#
  Loglike.burned<-Loglike[burn:iterations]#
	SteadyScore.burned<-SteadyScore$K0[burn:iterations]#
#
# make weights and Zs matrices#
Zs.burned<-t(sapply(Zs.burned, rbind))#
Ps.burned<-t(sapply(Ps.burned, cbind))#
#
	return(list(#
    Mu = Mu.burned,#
    Cov=Cov.burned,#
    Ps= Ps.burned,#
    Zs=Zs.burned,#
    k.occupied=SteadyScore.burned,#
    Log.likelihood=Loglike, y=y))#
}
test1<-Zmix_main(y[,1],#
          iterations=100,#
          k=5,#
          alphas= c(1/2^(c(6, 10, 30))),#
          burn=20,#
          init.method="random"#
          )
library("mvtnorm")#
library("dplyr")#
library("compiler")#
library("microbenchmark")#
library("ggplot2")#
library("mvnfast")#
#
Zmix_main<-function(#
          y,#
          iterations=2000,#
          k=10,#
          alphas= c(1/2^(c(6, 10, 30))),#
          burn=500,#
          init.method="Kmeans",#
          verbose=TRUE#
          ){#
#
      ## INNER FUNCTIONS#
      # Initial clustering#
      initiate_Z<-function(x, method=c("Kmeans", "random", "single") ){#
        if(method=="Kmeans"){#
        init.allocations<-  as.vector(kmeans(x, centers=k)$cluster)#
        } else if (method=="random"){#
          init.allocations<-  base::sample(c(1:k), n, replace=TRUE)#
        } else if (method=="single"){#
          init.allocations<-  rep(1, n)#
        }#
        return(init.allocations)#
      }#
      initiate_values<-function(ZZ){#
        IndiZ <-  (ZZ == matrix((1:k), nrow = n, ncol = k, byrow = T))#
        ns <-     apply(IndiZ,2,sum)  	#  size of each group#
        .Ysplit<- replicate(k, list())	#storage to group Y's#
        WkZ<-	    replicate(k, list(0))	#storage for within group variability#
        ybar<-	  replicate(k, list(0))#
        for (.i in 1:k){#
        if(r>1){  .Ysplit[[.i]]<-y[ZZ==.i,]#
        }else{ .Ysplit[[.i]]<-y[ZZ==.i]}#
          if (ns[.i]>1){					# for groups with >1 obsevations#
            ybar[[.i]]<- as.matrix(t(apply(.Ysplit[[.i]], 2, mean) ))  # UPDATE FOR UNIV#
            } else if (ns[.i]==1){#
              ybar[[.i]]<-	t( as.matrix(.Ysplit[[.i]]))#
              } else {#
                ybar[[.i]]<-	NA#
              }#
          #Within group unexplained variability#
          if (ns[.i]==0) {#
            WkZ[[.i]]<-	NA#
            } else if (ns[.i]==1){#
              WkZ[[.i]]<-crossprod(as.matrix(.Ysplit[[.i]]-ybar[[.i]]))#
              } else {#
                for ( .n in 1:ns[.i]){#
                  WkZ[[.i]]<-WkZ[[.i]]+ crossprod( .Ysplit[[.i]][.n,]-ybar[[.i]])#
                }#
              }#
            }#
            list('ns'=ns,'ybar'=ybar, 'WkZ'=WkZ)#
        }#
      MuSample<-function(covs, ns_mu, WkZ_mu, ybar_mu){#
          # covs<-matrix(covs, nrow=r, byrow=T)#
          if (ns_mu==0) {#
            rmvn(1, b0, covs/n0)#
            } else {#
              bk<-(n0/(ns_mu+n0))*b0+(ns_mu/(n0+ns_mu))*ybar_mu#
              Bk<-(1/(ns_mu+n0))*covs#
              rmvn(1, t(bk), Bk)#
            }#
        }#
      Update_probs<-function(Mu, Cov, Pi ){#
          update_group_Prob <-function(x){#
              a1<-dmvn(y, Mu[[x]], Cov[[x]])#
              a2<-a1*Pi[x]#
              # for (i in 1:n)		{#
              #   if (sum(PZs[i,])==0) {#
              #     PZs[i,]<-rep(1/k,k)    # if all probs are zero, randomly allocate obs. very rare, might lead to crap results#
              #   }}#
              return(a2)#
            }#
          # apply to each component#
          ugp<-mapply(update_group_Prob, c(1:k))#
          # scale each row#
          ugp/apply(ugp, 1, sum)#
        }#
      CovSample<-function(ns_cov, WkZ_cov, ybar_cov){#
          c_cov<-c0+ns_cov#
          if (ns_cov==0) {#
            MCMCpack::riwish(c0, C0)#
          } else {#
            C_cov<- C0 +((ns_cov*n0)/(ns_cov+n0)*crossprod(ybar_cov-b0)) +WkZ_cov#
            MCMCpack::riwish(c_cov,C_cov)#
          }#
        }#
      Update_Zs<-function(PZs){mapply(function(x) sample( (1:k), 1, prob=PZs[x,]), c(1:n))}#
      parallelAccept<-function(w1, w2, a1, a2){#
          w1[w1< 1e-200]<-1e-200   # truncate so super small values dont crash everyting#
          w2[w2< 1e-200]<-1e-200#
            T1<-dDirichlet(w2, a1, log=TRUE)#
            T2<-dDirichlet(w1, a2, log=TRUE)#
            B1<-dDirichlet(w1, a1, log=TRUE)#
            B2<-dDirichlet(w2, a2, log=TRUE)#
            MH<-min(1,	exp(T1+T2-B1-B2))#
          Ax<-sample(c(1,0), 1, prob=c(MH,1-MH))#
          return(Ax)}#
      dDirichlet<-function (x, alpha, log = FALSE) {#
          dlog = lgamma(sum(alpha)) + sum((alpha - 1) * log(x)) - sum(lgamma(alpha))#
          result = ifelse(!log, exp(dlog), dlog)#
          return(result)#
      	}#
      # function#
 rdirichlet<-cmpfun(rdirichlet)#
#
## compute basic values#
  nCh<- length(alphas)#
  if(is.vector(y)){#
    r<-1#
    n<-length(y)#
  } else {#
    r<-   dim(y)[2]#
    n<-   dim(y)[1]#
  }#
  Loglike <-   rep(0,iterations)#
  SteadyScore<-data.frame("Iteration"=c(1:iterations), "K0"=0)#
#
# hyperpriors#
  n0<-  tau <-1   # this is tau equiv#
  Ck<-	replicate(k, list())#
  c0<-  r+1#
if(r>1){#
  b0<-  apply(y,2,mean)#
  C0<-  0.75*cov(y)#
  d<-   sum(c(1:r))+r#
} else {#
  b0<-  mean(y)#
  C0<-  0.75*var(y)#
  d<-2#
}#
## parameters to estimate#
## storage structure  par[[chain]][[iterations]][[k]]#
  Ps<-     replicate(nCh, replicate(iterations, list()))#
  Mus<-    replicate(nCh, replicate(iterations, list()))#
  Covs<-   replicate(nCh, replicate(iterations, list()))#
  Zs<-     replicate(nCh, replicate(iterations, list()))#
#
  if(verbose==TRUE){pb <- txtProgressBar(min = 0, max = iterations, style = 3)}#
#
# FOR EACH ITERATION#
for (.it in 1:iterations){  #for each iteration#
  # TRACKER#
if(verbose==TRUE && .it %% 10 == 0) {#
  Sys.sleep(0.01)#
  par(mfrow=c(2,1))#
  plot(SteadyScore$K0~SteadyScore$Iteration, main='#non-empty groups', type='l')#
  ts.plot( t(sapply(Ps[[nCh]], rbind)), main='Target Weights', col=rainbow(k))#
  Sys.sleep(0)#
  setTxtProgressBar(pb, .it)#
}#
#
  for (.ch in 1:nCh){   #for each chain#
#
# Input values#
    if (.it==1){#
      # iteration=1, initiallize input values#
      init.Z<-  initiate_Z(y, init.method)#
      input.values<-    initiate_values(init.Z) # compute values needed#
        ns<-  input.values$ns#
        ybar<-input.values$ybar#
        WkZ<- input.values$WkZ#
      } else {#
      # Update given current pars#
      input.values<-initiate_values(Zs[[.ch]][[.it-1]])  # check me#
        ns<-    input.values$ns                         # check me#
        ybar<-  input.values$ybar                       # check me#
        WkZ<-   input.values$WkZ                        # check me#
      }#
#
      # STEP 2.1 : GENERATE Samples for WEIGHTS from DIRICHLET dist#
      Ps[[.ch]][[.it]] <- rdirichlet(m=1, par= ns+alphas[.ch])#
#
      # STEP 2.2 GENERATE Samples from Covariance#
      Covs[[.ch]][[.it]]<-mapply(CovSample, ns, WkZ, ybar, SIMPLIFY=FALSE)#
#
      # STEP 2.3 GENERATE SAMPLEs of Means#
      Mus[[.ch]][[.it]]<- mapply(MuSample,Covs[[.ch]][[.it]], ns, WkZ, ybar, SIMPLIFY=FALSE)#
#
      # STEP 3.1: Draw new classification probabilities:#
      PZs<-Update_probs(Mus[[.ch]][[.it]], Covs[[.ch]][[.it]], Ps[[.ch]][[.it]])#
#
      # STEP 3.2: Update allocations based on probabilitie#
      Zs[[.ch]][[.it]]<-Update_Zs(PZs)#
#
} # end chain loop#
## PRIOR PARALLEL TEMPERING#
if(.it>20 && runif(1)<.9){#
  Chain1<-sample( 1:(nCh-1), 1) ; 	Chain2<-Chain1+1#
  MHratio<- parallelAccept(Ps[[Chain1]][[.it]], Ps[[Chain2]][[.it]], rep(alphas[Chain1],k), rep(alphas[Chain2],k))#
  if (MHratio==1){#
  # Flip the allocations#
  .z1<-	Zs[[Chain1]][[.it]] 	;.z2<-	Zs[[Chain2]][[.it]]#
  Zs[[Chain1]][[.it]]<-.z2 	;Zs[[Chain2]][[.it]]<-.z1#
#
  #Mu#
  .mu1<-	Mus[[Chain1]][[.it]] 	;.mu2<-	Mus[[Chain2]][[.it]]#
  Mus[[Chain1]][[.it]]<-.mu2 	;Mus[[Chain2]][[.it]]<-.mu1#
#
  #Cov#
  .cv1<-	Covs[[Chain1]][[.it]] 	;.cv2<-	Covs[[Chain2]][[.it]]#
  Covs[[Chain1]][[.it]]<-.cv2 	;Covs[[Chain2]][[.it]]<-.cv1#
#
  #Ps#
  .p1<-	Ps[[Chain1]][[.it]] 	;.p2<-	Ps[[Chain2]][[.it]]#
  Ps[[Chain1]][[.it]]<-.p2 	;Ps[[Chain2]][[.it]]<-.p1#
}#
}#
#
# count number of occupied components#
SteadyScore$K0[.it]<- factor(Zs[[.ch]][[.it]]) %>% levels(.) %>% length(.)#
#
# compute Log Likelihood ( still need to optomize)#
for (i in 1:n){#
  non0id<-Zs[[nCh]][[.it]] %>% factor() %>% levels() %>% as.numeric()#
    .ll<-0#
    for (numK in 1:length(non0id)){#
       .ll<-.ll+#
       Ps[[nCh]][[.it]][non0id[numK]]*dmvn(#
         y[i,], Mus[[nCh]][[.it]][[ non0id[numK] ]],#
         Covs[[nCh]][[.it]][[ non0id[numK] ]])#
    }#
  Loglike[.it]<-Loglike[.it]+ log(.ll)#
}#
} # end iteration loop#
if(verbose==TRUE){close(pb)}#
#
  burn<-burn+1#
	nCh                                     #number of chains#
	Mu.burned<-Mus[[nCh]][burn:iterations]#
	Cov.burned<-Covs[[nCh]][burn:iterations]#
	Ps.burned<-Ps[[nCh]][burn:iterations]#
	Zs.burned<-Zs[[nCh]][burn:iterations]#
  Loglike.burned<-Loglike[burn:iterations]#
	SteadyScore.burned<-SteadyScore$K0[burn:iterations]#
#
# make weights and Zs matrices#
Zs.burned<-t(sapply(Zs.burned, rbind))#
Ps.burned<-t(sapply(Ps.burned, cbind))#
#
	return(list(#
    Mu = Mu.burned,#
    Cov=Cov.burned,#
    Ps= Ps.burned,#
    Zs=Zs.burned,#
    k.occupied=SteadyScore.burned,#
    Log.likelihood=Loglike, y=y))#
}
test1<-Zmix_main(y[,1],#
          iterations=100,#
          k=5,#
          alphas= c(1/2^(c(6, 10, 30))),#
          burn=20,#
          init.method="random"#
          )
library("mvtnorm")#
library("dplyr")#
library("compiler")#
library("microbenchmark")#
library("ggplot2")#
library("mvnfast")#
#
Zmix_main<-function(#
          y,#
          iterations=2000,#
          k=10,#
          alphas= c(1/2^(c(6, 10, 30))),#
          burn=500,#
          init.method="Kmeans",#
          verbose=TRUE#
          ){#
#
      ## INNER FUNCTIONS#
      # Initial clustering#
      initiate_Z<-function(x, method=c("Kmeans", "random", "single") ){#
        if(method=="Kmeans"){#
        init.allocations<-  as.vector(kmeans(x, centers=k)$cluster)#
        } else if (method=="random"){#
          init.allocations<-  base::sample(c(1:k), n, replace=TRUE)#
        } else if (method=="single"){#
          init.allocations<-  rep(1, n)#
        }#
        return(init.allocations)#
      }#
      initiate_values<-function(ZZ){#
        IndiZ <-  (ZZ == matrix((1:k), nrow = n, ncol = k, byrow = T))#
        ns <-     apply(IndiZ,2,sum)  	#  size of each group#
        .Ysplit<- replicate(k, list())	#storage to group Y's#
        WkZ<-	    replicate(k, list(0))	#storage for within group variability#
        ybar<-	  replicate(k, list(0))#
        for (.i in 1:k){#
        if(r>1){  .Ysplit[[.i]]<-y[ZZ==.i,]#
        }else{ .Ysplit[[.i]]<-y[ZZ==.i]}#
#
          if (ns[.i]>1){					# for groups with >1 obsevations#
            if(r>1){#
              ybar[[.i]]<- as.matrix(t(apply(.Ysplit[[.i]], 2, mean) ))  # UPDATE FOR UNIV#
            } else {#
            #  ybar[[.i]]<- as.matrix(t(apply(.Ysplit[[.i]], 2, mean) ))  # UPDATE FOR UNIV#
#
            }#
            } else if (ns[.i]==1){#
              ybar[[.i]]<-	t( as.matrix(.Ysplit[[.i]]))#
              } else {#
                ybar[[.i]]<-	NA#
              }#
          #Within group unexplained variability#
          if (ns[.i]==0) {#
            WkZ[[.i]]<-	NA#
            } else if (ns[.i]==1){#
              WkZ[[.i]]<-crossprod(as.matrix(.Ysplit[[.i]]-ybar[[.i]]))#
              } else {#
                for ( .n in 1:ns[.i]){#
                  WkZ[[.i]]<-WkZ[[.i]]+ crossprod( .Ysplit[[.i]][.n,]-ybar[[.i]])#
                }#
              }#
            }#
            list('ns'=ns,'ybar'=ybar, 'WkZ'=WkZ)#
        }#
      MuSample<-function(covs, ns_mu, WkZ_mu, ybar_mu){#
          # covs<-matrix(covs, nrow=r, byrow=T)#
          if (ns_mu==0) {#
            rmvn(1, b0, covs/n0)#
            } else {#
              bk<-(n0/(ns_mu+n0))*b0+(ns_mu/(n0+ns_mu))*ybar_mu#
              Bk<-(1/(ns_mu+n0))*covs#
              rmvn(1, t(bk), Bk)#
            }#
        }#
      Update_probs<-function(Mu, Cov, Pi ){#
          update_group_Prob <-function(x){#
              a1<-dmvn(y, Mu[[x]], Cov[[x]])#
              a2<-a1*Pi[x]#
              # for (i in 1:n)		{#
              #   if (sum(PZs[i,])==0) {#
              #     PZs[i,]<-rep(1/k,k)    # if all probs are zero, randomly allocate obs. very rare, might lead to crap results#
              #   }}#
              return(a2)#
            }#
          # apply to each component#
          ugp<-mapply(update_group_Prob, c(1:k))#
          # scale each row#
          ugp/apply(ugp, 1, sum)#
        }#
      CovSample<-function(ns_cov, WkZ_cov, ybar_cov){#
          c_cov<-c0+ns_cov#
          if (ns_cov==0) {#
            MCMCpack::riwish(c0, C0)#
          } else {#
            C_cov<- C0 +((ns_cov*n0)/(ns_cov+n0)*crossprod(ybar_cov-b0)) +WkZ_cov#
            MCMCpack::riwish(c_cov,C_cov)#
          }#
        }#
      Update_Zs<-function(PZs){mapply(function(x) sample( (1:k), 1, prob=PZs[x,]), c(1:n))}#
      parallelAccept<-function(w1, w2, a1, a2){#
          w1[w1< 1e-200]<-1e-200   # truncate so super small values dont crash everyting#
          w2[w2< 1e-200]<-1e-200#
            T1<-dDirichlet(w2, a1, log=TRUE)#
            T2<-dDirichlet(w1, a2, log=TRUE)#
            B1<-dDirichlet(w1, a1, log=TRUE)#
            B2<-dDirichlet(w2, a2, log=TRUE)#
            MH<-min(1,	exp(T1+T2-B1-B2))#
          Ax<-sample(c(1,0), 1, prob=c(MH,1-MH))#
          return(Ax)}#
      dDirichlet<-function (x, alpha, log = FALSE) {#
          dlog = lgamma(sum(alpha)) + sum((alpha - 1) * log(x)) - sum(lgamma(alpha))#
          result = ifelse(!log, exp(dlog), dlog)#
          return(result)#
      	}#
      # function#
 rdirichlet<-cmpfun(rdirichlet)#
#
## compute basic values#
  nCh<- length(alphas)#
  if(is.vector(y)){#
    r<-1#
    n<-length(y)#
  } else {#
    r<-   dim(y)[2]#
    n<-   dim(y)[1]#
  }#
  Loglike <-   rep(0,iterations)#
  SteadyScore<-data.frame("Iteration"=c(1:iterations), "K0"=0)#
#
# hyperpriors#
  n0<-  tau <-1   # this is tau equiv#
  Ck<-	replicate(k, list())#
  c0<-  r+1#
if(r>1){#
  b0<-  apply(y,2,mean)#
  C0<-  0.75*cov(y)#
  d<-   sum(c(1:r))+r#
} else {#
  b0<-  mean(y)#
  C0<-  0.75*var(y)#
  d<-2#
}#
## parameters to estimate#
## storage structure  par[[chain]][[iterations]][[k]]#
  Ps<-     replicate(nCh, replicate(iterations, list()))#
  Mus<-    replicate(nCh, replicate(iterations, list()))#
  Covs<-   replicate(nCh, replicate(iterations, list()))#
  Zs<-     replicate(nCh, replicate(iterations, list()))#
#
  if(verbose==TRUE){pb <- txtProgressBar(min = 0, max = iterations, style = 3)}#
#
# FOR EACH ITERATION#
for (.it in 1:iterations){  #for each iteration#
  # TRACKER#
if(verbose==TRUE && .it %% 10 == 0) {#
  Sys.sleep(0.01)#
  par(mfrow=c(2,1))#
  plot(SteadyScore$K0~SteadyScore$Iteration, main='#non-empty groups', type='l')#
  ts.plot( t(sapply(Ps[[nCh]], rbind)), main='Target Weights', col=rainbow(k))#
  Sys.sleep(0)#
  setTxtProgressBar(pb, .it)#
}#
#
  for (.ch in 1:nCh){   #for each chain#
#
# Input values#
    if (.it==1){#
      # iteration=1, initiallize input values#
      init.Z<-  initiate_Z(y, init.method)#
      input.values<-    initiate_values(init.Z) # compute values needed#
        ns<-  input.values$ns#
        ybar<-input.values$ybar#
        WkZ<- input.values$WkZ#
      } else {#
      # Update given current pars#
      input.values<-initiate_values(Zs[[.ch]][[.it-1]])  # check me#
        ns<-    input.values$ns                         # check me#
        ybar<-  input.values$ybar                       # check me#
        WkZ<-   input.values$WkZ                        # check me#
      }#
#
      # STEP 2.1 : GENERATE Samples for WEIGHTS from DIRICHLET dist#
      Ps[[.ch]][[.it]] <- rdirichlet(m=1, par= ns+alphas[.ch])#
#
      # STEP 2.2 GENERATE Samples from Covariance#
      Covs[[.ch]][[.it]]<-mapply(CovSample, ns, WkZ, ybar, SIMPLIFY=FALSE)#
#
      # STEP 2.3 GENERATE SAMPLEs of Means#
      Mus[[.ch]][[.it]]<- mapply(MuSample,Covs[[.ch]][[.it]], ns, WkZ, ybar, SIMPLIFY=FALSE)#
#
      # STEP 3.1: Draw new classification probabilities:#
      PZs<-Update_probs(Mus[[.ch]][[.it]], Covs[[.ch]][[.it]], Ps[[.ch]][[.it]])#
#
      # STEP 3.2: Update allocations based on probabilitie#
      Zs[[.ch]][[.it]]<-Update_Zs(PZs)#
#
} # end chain loop#
## PRIOR PARALLEL TEMPERING#
if(.it>20 && runif(1)<.9){#
  Chain1<-sample( 1:(nCh-1), 1) ; 	Chain2<-Chain1+1#
  MHratio<- parallelAccept(Ps[[Chain1]][[.it]], Ps[[Chain2]][[.it]], rep(alphas[Chain1],k), rep(alphas[Chain2],k))#
  if (MHratio==1){#
  # Flip the allocations#
  .z1<-	Zs[[Chain1]][[.it]] 	;.z2<-	Zs[[Chain2]][[.it]]#
  Zs[[Chain1]][[.it]]<-.z2 	;Zs[[Chain2]][[.it]]<-.z1#
#
  #Mu#
  .mu1<-	Mus[[Chain1]][[.it]] 	;.mu2<-	Mus[[Chain2]][[.it]]#
  Mus[[Chain1]][[.it]]<-.mu2 	;Mus[[Chain2]][[.it]]<-.mu1#
#
  #Cov#
  .cv1<-	Covs[[Chain1]][[.it]] 	;.cv2<-	Covs[[Chain2]][[.it]]#
  Covs[[Chain1]][[.it]]<-.cv2 	;Covs[[Chain2]][[.it]]<-.cv1#
#
  #Ps#
  .p1<-	Ps[[Chain1]][[.it]] 	;.p2<-	Ps[[Chain2]][[.it]]#
  Ps[[Chain1]][[.it]]<-.p2 	;Ps[[Chain2]][[.it]]<-.p1#
}#
}#
#
# count number of occupied components#
SteadyScore$K0[.it]<- factor(Zs[[.ch]][[.it]]) %>% levels(.) %>% length(.)#
#
# compute Log Likelihood ( still need to optomize)#
for (i in 1:n){#
  non0id<-Zs[[nCh]][[.it]] %>% factor() %>% levels() %>% as.numeric()#
    .ll<-0#
    for (numK in 1:length(non0id)){#
       .ll<-.ll+#
       Ps[[nCh]][[.it]][non0id[numK]]*dmvn(#
         y[i,], Mus[[nCh]][[.it]][[ non0id[numK] ]],#
         Covs[[nCh]][[.it]][[ non0id[numK] ]])#
    }#
  Loglike[.it]<-Loglike[.it]+ log(.ll)#
}#
} # end iteration loop#
if(verbose==TRUE){close(pb)}#
#
  burn<-burn+1#
	nCh                                     #number of chains#
	Mu.burned<-Mus[[nCh]][burn:iterations]#
	Cov.burned<-Covs[[nCh]][burn:iterations]#
	Ps.burned<-Ps[[nCh]][burn:iterations]#
	Zs.burned<-Zs[[nCh]][burn:iterations]#
  Loglike.burned<-Loglike[burn:iterations]#
	SteadyScore.burned<-SteadyScore$K0[burn:iterations]#
#
# make weights and Zs matrices#
Zs.burned<-t(sapply(Zs.burned, rbind))#
Ps.burned<-t(sapply(Ps.burned, cbind))#
#
	return(list(#
    Mu = Mu.burned,#
    Cov=Cov.burned,#
    Ps= Ps.burned,#
    Zs=Zs.burned,#
    k.occupied=SteadyScore.burned,#
    Log.likelihood=Loglike, y=y))#
}
test1<-Zmix_main(y,#
          iterations=100,#
          k=5,#
          alphas= c(1/2^(c(6, 10, 30))),#
          burn=20,#
          init.method="random"#
          )
IndiZ <-  (ZZ == matrix((1:k), nrow = n, ncol = k, byrow = T))
ns <-     apply(IndiZ,2,sum)  	#  size of each group
.Ysplit<- replicate(k, list())	#storage to group Y's
install.packages(c("devtools", "roxygen2", "testthat", "knitr"))
library(devtools)#
has_devel()
getwd()
devtools::create("/Users/zoevanhavre/Documents/Zdev")
devtools::use_package("dplyr")
setwd("Documents/Zdev/")
devtools::use_package("dplyr")
devtools::use_package("ggplot")
devtools::use_package("ggplot2")
install.packages("tibble")
devtools::use_package("tible")
devtools::use_package("tibble")
build()
devtools::load_all()
?rdirichlet
?rdirichlet
devtools::document()
?rdirichlet
devtools::document()
devtools::document()
?rdirichlet
rdirichlet(c(1,1))
rdirichlet(par=c(1,1))
rdirichlet(c(1,1))
devtools::document()
rdirichlet(c(1,1))
?rdirichlet
devtools::document()
?rdirichlet
devtools::document()
?rdirichlet
!FALSE
devtools::document()
devtools::document()
?dDirichlet
dDirichlet(c(.1, .9), c(0.1,0.1))
dDirichlet(c(.1, .9), c(0.1,0.1))
devtools::document()
build()
build()
